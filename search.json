[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mistinguette",
    "section": "",
    "text": "pip install mistinguette",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "mistinguette",
    "section": "",
    "text": "pip install mistinguette",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "mistinguette",
    "section": "Getting started",
    "text": "Getting started\nOpenAI’s Python SDK will automatically be installed with Mistinguette, if you don’t already have it.\n\nfrom mistinguette import *\n\nMistinguette only exports the symbols that are needed to use the library, so you can use import * to import them. Alternatively, just use:\nimport mistinguette\n…and then add the prefix mistinguette. to any usages of the module.\n\n\nmodels\n\n['codestral-2501',\n 'mistral-large-2411',\n 'pixtral-large-2411',\n 'mistral-saba-2502',\n 'ministral-3b-2410',\n 'ministral-8b-2410',\n 'mistral-embed-2312',\n 'mistral-moderation-2411',\n 'mistral-ocr-2503',\n 'mistral-small-2503',\n 'open-mistral-nemo-2407']\n\n\nFor these examples, we’ll use mistral-large-2411.\n\nmodel = models[1]",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "mistinguette",
    "section": "Chat",
    "text": "Chat\nThe main interface to Mistinguette is the Chat class, which provides a stateful interface to the models:\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat(\"I'm Jeremy\")\n\nHello Jeremy! Nice to meet you. How can I assist you today?\n\n\nid: 401b34940c76413b93fde428839d2f28\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=18 completion_tokens=16 total_tokens=34\ncreated: 1743589362\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘Hello Jeremy! Nice to meet you. How can I assist you today?’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nr = chat(\"What's my name?\")\nr\n\nJeremy\n\n\nid: 851227e58cbe4e6b97751897578ec3e5\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=50 completion_tokens=2 total_tokens=52\ncreated: 1743589366\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘Jeremy’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nAs you see above, displaying the results of a call in a notebook shows just the message contents, with the other details hidden behind a collapsible section. Alternatively you can print the details:\n\nprint(r)\n\nid='851227e58cbe4e6b97751897578ec3e5' object='chat.completion' model='mistral-large-2411' usage=In: 50; Out: 2; Total: 52 created=1743589366 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Jeremy', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n\n\nYou can use stream=True to stream the results as soon as they arrive (although you will only see the gradual generation if you execute the notebook yourself, of course!)\n\nfor o in chat(\"What's your name?\", stream=True): print(o, end='')\n\nI don't have a name. I'm here to assist you. Is there something specific you would like help with?",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "index.html#model-capabilities",
    "href": "index.html#model-capabilities",
    "title": "mistinguette",
    "section": "Model Capabilities",
    "text": "Model Capabilities\nDifferent Mistral AI models have different capabilities. For instance, mistral-large-2411 can not take an image as input as opposed to pixtral-large-2411:\n\n# o1 does not support streaming or setting the temperature\nm = \"mistral-large-2411\"\ncan_stream(m), can_set_system_prompt(m), can_set_temperature(m), can_use_image(m)\n\n(True, True, True, False)",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "index.html#tool-use",
    "href": "index.html#tool-use",
    "title": "mistinguette",
    "section": "Tool use",
    "text": "Tool use\nTool use lets the model use external tools.\nWe use docments to make defining Python functions as ergonomic as possible. Each parameter (and the return value) should have a type, and a docments comment with the description of what it is. As an example we’ll write a simple function that adds numbers together, and will tell us when it’s being called:\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int=1 # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\nSometimes the model will say something like “according to the sums tool the answer is” – generally we’d rather it just tells the user the answer, so we can use a system prompt to help with this:\n\nsp = \"Never mention what tools you use.\"\n\nWe’ll get the model to add up some long numbers:\n\nmodel\n\n'mistral-large-2411'\n\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\nTo use tools, pass a list of them to Chat:\n\nchat = Chat(model, sp=sp, tools=[sums])\n\nNow when we call that with our prompt, the model doesn’t return the answer, but instead returns a tool_use message, which means we have to call the named tool with the provided parameters:\n\nr = chat(pr)\nr\n\nFinding the sum of 604542 and 6458932\n\n\n\nid: add10a17e8494845a76595e45d765ba7\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=132 completion_tokens=37 total_tokens=169\ncreated: 1743589453\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=’‘, tool_calls=[ToolCall(function=FunctionCall(name=’sums’, arguments=’{“a”: 604542, “b”: 6458932}’), id=’TLVUDh6Me’, type=None, index=0)], prefix=False, role=’assistant’), finish_reason=‘tool_calls’)]\n\n\n\nMistinguette handles all that for us – we just have to pass along the message, and it all happens automatically:\n\nchat()\n\nWhat is 604542+6458932? The answer is 7,063,474.\n\n\nid: d4c05d5724fa433cb9020371edc15f97\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=197 completion_tokens=33 total_tokens=230\ncreated: 1743589467\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘What is 604542+6458932? The answer is 7,063,474.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nYou can see how many tokens have been used at any time by checking the use property.\n\nchat.use\n\nIn: 329; Out: 70; Total: 399\n\n\n\nTool loop\nWe can do everything needed to use tools in a single step, by using Chat.toolloop. This can even call multiple tools as needed solve a problem. For example, let’s define a tool to handle multiplication:\n\ndef mults(\n    a:int,  # First thing to multiply\n    b:int=1 # Second thing to multiply\n) -&gt; int: # The product of the inputs\n    \"Multiplies a * b.\"\n    print(f\"Finding the product of {a} and {b}\")\n    return a * b\n\nNow with a single call we can calculate (a+b)*2 – by passing show_trace we can see each response from the model in the process:\n\nchat = Chat(model, sp=sp, tools=[sums,mults])\npr = f'Calculate ({a}+{b})*2'\npr\n\n'Calculate (604542+6458932)*2'\n\n\n\ndef pchoice(r): print(r.choices[0])\n\n\nr = chat.toolloop(pr, trace_func=pchoice)\n\nFinding the sum of 604542 and 6458932\nFinding the product of 2 and 7103474\nChoice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Sfet73hgfRtSI2N25K97D7tO', function=Function(arguments='{\"a\": 604542, \"b\": 6458932}', name='sums'), type='function'), ChatCompletionMessageToolCall(id='call_mFQNJgjATAI2pYFFQyvfg0W2', function=Function(arguments='{\"a\": 2, \"b\": 7103474}', name='mults'), type='function')]))\nChoice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The result of \\\\((604542 + 6458932) \\\\times 2\\\\) is 14,206,948.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))\n\n\nOpenAI uses special tags for math equations, which we can replace using wrap_latex:\n\nwrap_latex(contents(r))\n\nThe result of \\((604542 + 6458932) \\times 2\\) is 14,206,948.",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "index.html#images",
    "href": "index.html#images",
    "title": "mistinguette",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\nfn = Path('samples/puppy.jpg')\ndisplay.Image(filename=fn, width=200)\n\n\n\n\n\n\n\n\nWe create a Chat object as before:\n\nmodel = \"pixtral-large-2411\"\n\n\nchat = Chat(model)\n\nMistinguette expects images as a list of bytes, so we read in the file:\n\nimg = fn.read_bytes()\n\nPrompts to Claudia can be lists, containing text, images, or both, eg:\n\nchat([img, \"In brief, what color flowers are in this image?\"])\n\nThe flowers in the image are purple. These purple flowers seem to be daisy-like, and they are growing near the puppy, which is lying on the grass.\n\n\nid: 110860448b5f443588a5288fbe72a807\nobject: chat.completion\nmodel: pixtral-large-2411\nusage: prompt_tokens=274 completion_tokens=36 total_tokens=310\ncreated: 1743589622\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘The flowers in the image are purple. These purple flowers seem to be daisy-like, and they are growing near the puppy, which is lying on the grass.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nThe image is included as input tokens.\n\nchat.use\n\nIn: 274; Out: 36; Total: 310\n\n\nAlternatively, Mistinguette supports creating a multi-stage chat with separate image and text prompts. For instance, you can pass just the image as the initial prompt (in which case the model will make some general comments about what it sees), and then follow up with questions in additional prompts:\n\nchat = Chat(model)\nchat(img)\n\nThis is an adorable puppy! It looks like a Cavalier King Charles Spaniel, known for its friendly and affectionate nature. These dogs are great companions and enjoy being around people. They have a gentle temperament and are well-suited to indoor living. If you have any specific questions about this breed or puppies in general, feel free to ask!\n\n\nid: 47c88a7e27004d3c832a5dc8277f30a4\nobject: chat.completion\nmodel: pixtral-large-2411\nusage: prompt_tokens=263 completion_tokens=76 total_tokens=339\ncreated: 1743589646\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘This is an adorable puppy! It looks like a Cavalier King Charles Spaniel, known for its friendly and affectionate nature. These dogs are great companions and enjoy being around people. They have a gentle temperament and are well-suited to indoor living. If you have any specific questions about this breed or puppies in general, feel free to ask!’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nchat('What direction is the puppy facing?')\n\nThe puppy is facing the camera, looking directly at it. This means it is facing forward from the perspective of the viewer.\n\n\nid: 118c686b1af64f42b04ff506a7e3f694\nobject: chat.completion\nmodel: pixtral-large-2411\nusage: prompt_tokens=350 completion_tokens=27 total_tokens=377\ncreated: 1743589654\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘The puppy is facing the camera, looking directly at it. This means it is facing forward from the perspective of the viewer.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nchat('What color is it?')\n\nThe puppy is a Cavalier King Charles Spaniel with a Blenheim coloring. This means it has a white coat with chestnut or reddish-brown markings, particularly on the ears and parts of the face. The spot on the top of its head is a characteristic feature of the Blenheim color pattern.\n\n\nid: 9e7c3903c05740a2845e8dca2307428c\nobject: chat.completion\nmodel: pixtral-large-2411\nusage: prompt_tokens=385 completion_tokens=69 total_tokens=454\ncreated: 1743589657\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘The puppy is a Cavalier King Charles Spaniel with a Blenheim coloring. This means it has a white coat with chestnut or reddish-brown markings, particularly on the ears and parts of the face. The spot on the top of its head is a characteristic feature of the Blenheim color pattern.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nNote that the image is passed in again for every input in the dialog, so that number of input tokens increases quickly with this kind of chat.\n\nchat.use\n\nIn: 998; Out: 172; Total: 1170",
    "crumbs": [
      "mistinguette"
    ]
  },
  {
    "objectID": "toolloop.html",
    "href": "toolloop.html",
    "title": "Tool loop",
    "section": "",
    "text": "model = models[0]\n\n\norders = {\n    \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n    \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n    \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n\ncustomers = {\n    \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n               orders=[orders['O1'], orders['O2']]),\n    \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n               orders=[orders['O3']])\n}\n\n\ndef get_customer_info(\n    customer_id:str # ID of the customer\n): # Customer's name, email, phone number, and list of orders\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_order_details(\n    order_id:str # ID of the order\n): # Order's ID, product name, quantity, price, and order status\n    \"Retrieves the details of a specific order based on the order ID\"\n    print(f'- Retrieving order {order_id}')\n    return orders.get(order_id, \"Order not found\")\n\ndef cancel_order(\n    order_id:str # ID of the order to cancel\n)-&gt;bool: # True if the cancellation is successful\n    \"Cancels an order based on the provided order ID\"\n    print(f'- Cancelling order {order_id}')\n    if order_id not in orders: return False\n    orders[order_id]['status'] = 'Cancelled'\n    return True\n\n\ntools = [get_customer_info, get_order_details, cancel_order]\nchat = Chat(model, tools=tools)\n\n\nr = chat('Can you tell me the email address for customer C2?')\n\n- Retrieving customer C2\n\n\n\nchoice = r.choices[0]\nprint(choice.finish_reason)\nchoice\n\ntool_calls\n\n\nChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='get_customer_info', arguments='{\"customer_id\": \"C2\"}'), id='5x9gdVe3v', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')\n\n\n\nr = chat()\nr\n\nThe email address for customer C2 is jane@example.com.\n\n\nid: 3d09df23ad184c85a3206b78b0e70faf\nobject: chat.completion\nmodel: codestral-2501\nusage: prompt_tokens=376 completion_tokens=14 total_tokens=390\ncreated: 1743590196\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘The email address for customer C2 is jane@example.com.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nchat = Chat(model, tools=tools)\nr = chat('Please cancel all orders for customer C1 for me.')\nprint(r.choices[0].finish_reason)\nfind_block(r)\n\n- Retrieving customer C1\ntool_calls\n\n\nAssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='get_customer_info', arguments='{\"customer_id\": \"C1\"}'), id='hjIMAgj4g', type=None, index=0)], prefix=False, role='assistant')\n\n\n\nsource\n\nChat.toolloop\n\n Chat.toolloop (pr, max_steps=10, trace_func:Optional[&lt;built-\n                infunctioncallable&gt;]=None, cont_func:Optional[&lt;built-\n                infunctioncallable&gt;]=&lt;function noop&gt;,\n                temperature:OptionalNullable[float]=Unset(),\n                top_p:Optional[float]=None,\n                max_tokens:OptionalNullable[int]=Unset(),\n                stream:Optional[bool]=False,\n                stop:Union[Stop,StopTypedDict,NoneType]=None,\n                random_seed:OptionalNullable[int]=Unset(), response_format\n                :Union[mistralai.models.responseformat.ResponseFormat,mist\n                ralai.models.responseformat.ResponseFormatTypedDict,NoneTy\n                pe]=None, tools:OptionalNullable[typing.Union[typing.List[\n                mistralai.models.tool.Tool],typing.List[mistralai.models.t\n                ool.ToolTypedDict]]]=Unset(), tool_choice:Union[ChatComple\n                tionRequestToolChoice,ChatCompletionRequestToolChoiceTyped\n                Dict,NoneType]=None,\n                presence_penalty:Optional[float]=None,\n                frequency_penalty:Optional[float]=None,\n                n:OptionalNullable[int]=Unset(), prediction:Union[mistrala\n                i.models.prediction.Prediction,mistralai.models.prediction\n                .PredictionTypedDict,NoneType]=None,\n                parallel_tool_calls:Optional[bool]=None,\n                safe_prompt:Optional[bool]=None, retries:OptionalNullable[\n                mistralai.utils.retries.RetryConfig]=Unset(),\n                server_url:Optional[str]=None,\n                timeout_ms:Optional[int]=None,\n                http_headers:Optional[Mapping[str,str]]=None)\n\nAdd prompt pr to dialog and get a response from the model, automatically following up with tool_use messages\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\n\n\nPrompt to pass to model\n\n\nmax_steps\nint\n10\nMaximum number of tool requests to loop through\n\n\ntrace_func\nOptional\nNone\nFunction to trace tool use steps (e.g print)\n\n\ncont_func\nOptional\nnoop\nFunction that stops loop if returns False\n\n\ntemperature\nOptionalNullable\n\n\n\n\ntop_p\nOptional\nNone\n\n\n\nmax_tokens\nOptionalNullable\n\n\n\n\nstream\nOptional\nFalse\n\n\n\nstop\nUnion\nNone\n\n\n\nrandom_seed\nOptionalNullable\n\n\n\n\nresponse_format\nUnion\nNone\n\n\n\ntools\nOptionalNullable\n\n\n\n\ntool_choice\nUnion\nNone\n\n\n\npresence_penalty\nOptional\nNone\n\n\n\nfrequency_penalty\nOptional\nNone\n\n\n\nn\nOptionalNullable\n\n\n\n\nprediction\nUnion\nNone\n\n\n\nparallel_tool_calls\nOptional\nNone\n\n\n\nsafe_prompt\nOptional\nNone\n\n\n\nretries\nOptionalNullable\n\n\n\n\nserver_url\nOptional\nNone\n\n\n\ntimeout_ms\nOptional\nNone\n\n\n\nhttp_headers\nOptional\nNone\n\n\n\n\n\n\nExported source\n@patch\n@delegates(mistralai.Chat.complete)\ndef toolloop(self:Chat,\n             pr, # Prompt to pass to model\n             max_steps=10, # Maximum number of tool requests to loop through\n             trace_func:Optional[callable]=None, # Function to trace tool use steps (e.g `print`)\n             cont_func:Optional[callable]=noop, # Function that stops loop if returns False\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response from the model, automatically following up with `tool_use` messages\"\n    import time\n    r = self(pr, **kwargs)\n    for i in range(max_steps):\n        ch = r.choices[0]\n        if ch.finish_reason!='tool_calls': break\n        if trace_func: trace_func(r)\n        time.sleep(1)  # Add 1 second pause between queries to avoid rate limiting\n        r = self(**kwargs)\n        if not (cont_func or noop)(self.h[-2]): break\n    if trace_func: trace_func(r)\n    return r\n\n\n\nchat = Chat(model, tools=tools)\nr = chat.toolloop('Please cancel all orders for customer C1 for me.', trace_func=print)\nr\n\n- Retrieving customer C1\nid='7fd7c25aba0f4b199b5a55a658609d4d' object='chat.completion' model='codestral-2501' usage=In: 253; Out: 23; Total: 276 created=1743590326 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='get_customer_info', arguments='{\"customer_id\": \"C1\"}'), id='cRrdLFWDH', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')]\n- Cancelling order O1\n- Cancelling order O2\nid='8cfd03d225394b488b2ea8b7269b0d86' object='chat.completion' model='codestral-2501' usage=In: 408; Out: 41; Total: 449 created=1743590328 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='cancel_order', arguments='{\"order_id\": \"O1\"}'), id='CoejqXYQw', type=None, index=0), ToolCall(function=FunctionCall(name='cancel_order', arguments='{\"order_id\": \"O2\"}'), id='GiYzxpY3Y', type=None, index=1)], prefix=False, role='assistant'), finish_reason='tool_calls')]\nid='321e089641684d5a921a81b0cb4ba5f5' object='chat.completion' model='codestral-2501' usage=In: 494; Out: 11; Total: 505 created=1743590329 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='All orders for customer C1 have been cancelled.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n\n\nAll orders for customer C1 have been cancelled.\n\n\nid: 321e089641684d5a921a81b0cb4ba5f5\nobject: chat.completion\nmodel: codestral-2501\nusage: prompt_tokens=494 completion_tokens=11 total_tokens=505\ncreated: 1743590329\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘All orders for customer C1 have been cancelled.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nchat.toolloop('What is the status of order O2?')\n\n- Retrieving order O2\n\n\nThe status of order O2 is Cancelled.\n\n\nid: 1ac07a89c98c4495af308cb7011587ac\nobject: chat.completion\nmodel: codestral-2501\nusage: prompt_tokens=597 completion_tokens=11 total_tokens=608\ncreated: 1743590335\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘The status of order O2 is Cancelled.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Mistinguette’s source",
    "section": "",
    "text": "source\n\n\n\n can_use_image (m)\n\n\n\nExported source\nmodel_types = {\n    # Premier models\n    'codestral-2501': 'codestral-latest', # code generation model\n    'mistral-large-2411': 'mistral-large-latest', # top-tier reasoning model for high-complexity tasks\n    'pixtral-large-2411': 'pixtral-large-latest', # frontier-class multimodal model\n    'mistral-saba-2502': 'mistral-saba-latest', # model for languages from the Middle East and South Asia\n    'ministral-3b-2410': 'ministral-3b-latest', # edge model\n    'ministral-8b-2410': 'ministral-8b-latest', # edge model with high performance/price ratio\n    'mistral-embed-2312': 'mistral-embed', # embedding model\n    'mistral-moderation-2411': 'mistral-moderation-latest', # moderation service to detect harmful text content\n    'mistral-ocr-2503': 'mistral-ocr-latest', # OCR model to extract interleaved text and images\n    \n    # Free models (with weight availability)\n    'mistral-small-2503': 'mistral-small-latest', # small model with image understanding capabilities\n    \n    # Research models\n    'open-mistral-nemo-2407': 'open-mistral-nemo', # multilingual open source model\n}\n\n\n\n\nExported source\nall_models = list(model_types)\n\n\n\n\nExported source\nvision_models = ['pixtral-large-2411', 'mistral-small-2503', 'mistral-ocr-2503']\n\n\n\n\nExported source\nembed_models = ['mistral-embed-2312']\n\n\n\n\nExported source\nocr_models = ['mistral-ocr-2503']\n\n\n\n\nExported source\ntool_models = ['codestral-2501', 'mistral-large-2411', \n               'mistral-small-2503', 'ministral-3b-2410', 'ministral-8b-2410',\n               'pixtral-large-2411', 'open-mistral-nemo-2407']\n\n\n\n\nExported source\ntext_only_models = set(all_models) - set(vision_models) - set(embed_models) - set(ocr_models)\n\n\n\n\nExported source\nhas_streaming_models = set(all_models) - set(embed_models) - set(ocr_models)\nhas_system_prompt_models = set(all_models) - set(embed_models) - set(ocr_models)\nhas_temperature_models = set(all_models) - set(embed_models) - set(ocr_models)\n\n\n\n\nExported source\ndef can_stream(m): return m in has_streaming_models\ndef can_set_system_prompt(m): return m in has_system_prompt_models\ndef can_set_temperature(m): return m in has_temperature_models\ndef can_use_image(m): return m in vision_models\n\n\n\nsource\n\n\n\n\n can_set_temperature (m)\n\n\nsource\n\n\n\n\n can_set_system_prompt (m)\n\n\nsource\n\n\n\n\n can_stream (m)\n\n\n# all models except codestral-mamba support custom structured outputs\n\n\nmodel = models[1]; model\n\n'mistral-large-2411'",
    "crumbs": [
      "Mistinguette's source"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Mistinguette’s source",
    "section": "",
    "text": "source\n\n\n\n can_use_image (m)\n\n\n\nExported source\nmodel_types = {\n    # Premier models\n    'codestral-2501': 'codestral-latest', # code generation model\n    'mistral-large-2411': 'mistral-large-latest', # top-tier reasoning model for high-complexity tasks\n    'pixtral-large-2411': 'pixtral-large-latest', # frontier-class multimodal model\n    'mistral-saba-2502': 'mistral-saba-latest', # model for languages from the Middle East and South Asia\n    'ministral-3b-2410': 'ministral-3b-latest', # edge model\n    'ministral-8b-2410': 'ministral-8b-latest', # edge model with high performance/price ratio\n    'mistral-embed-2312': 'mistral-embed', # embedding model\n    'mistral-moderation-2411': 'mistral-moderation-latest', # moderation service to detect harmful text content\n    'mistral-ocr-2503': 'mistral-ocr-latest', # OCR model to extract interleaved text and images\n    \n    # Free models (with weight availability)\n    'mistral-small-2503': 'mistral-small-latest', # small model with image understanding capabilities\n    \n    # Research models\n    'open-mistral-nemo-2407': 'open-mistral-nemo', # multilingual open source model\n}\n\n\n\n\nExported source\nall_models = list(model_types)\n\n\n\n\nExported source\nvision_models = ['pixtral-large-2411', 'mistral-small-2503', 'mistral-ocr-2503']\n\n\n\n\nExported source\nembed_models = ['mistral-embed-2312']\n\n\n\n\nExported source\nocr_models = ['mistral-ocr-2503']\n\n\n\n\nExported source\ntool_models = ['codestral-2501', 'mistral-large-2411', \n               'mistral-small-2503', 'ministral-3b-2410', 'ministral-8b-2410',\n               'pixtral-large-2411', 'open-mistral-nemo-2407']\n\n\n\n\nExported source\ntext_only_models = set(all_models) - set(vision_models) - set(embed_models) - set(ocr_models)\n\n\n\n\nExported source\nhas_streaming_models = set(all_models) - set(embed_models) - set(ocr_models)\nhas_system_prompt_models = set(all_models) - set(embed_models) - set(ocr_models)\nhas_temperature_models = set(all_models) - set(embed_models) - set(ocr_models)\n\n\n\n\nExported source\ndef can_stream(m): return m in has_streaming_models\ndef can_set_system_prompt(m): return m in has_system_prompt_models\ndef can_set_temperature(m): return m in has_temperature_models\ndef can_use_image(m): return m in vision_models\n\n\n\nsource\n\n\n\n\n can_set_temperature (m)\n\n\nsource\n\n\n\n\n can_set_system_prompt (m)\n\n\nsource\n\n\n\n\n can_stream (m)\n\n\n# all models except codestral-mamba support custom structured outputs\n\n\nmodel = models[1]; model\n\n'mistral-large-2411'",
    "crumbs": [
      "Mistinguette's source"
    ]
  },
  {
    "objectID": "core.html#mistral-sdk",
    "href": "core.html#mistral-sdk",
    "title": "Mistinguette’s source",
    "section": "Mistral SDK",
    "text": "Mistral SDK\n\ncli = Mistral(api_key=MISTRAL_API_KEY)\n\nThis is what Mistral’s SDK provides for interacting with Python. To use it, pass it a list of messages, with content and a role. The roles should alternate between user and assistant.\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What's in this image?\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": \"https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg\"\n            }\n        ]\n    }\n]\n\n# Get the chat response\nchat_response = cli.chat.complete(\n    model='pixtral-large-2411',\n    messages=messages\n)\n\n\nchat_response\n\nChatCompletionResponse(id='7b394afb791f4b00afba2da0f7e23814', object='chat.completion', model='pixtral-large-2411', usage=UsageInfo(prompt_tokens=1360, completion_tokens=133, total_tokens=1493), created=1743585871, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"The image depicts the Eiffel Tower in Paris, France, on a snowy winter day. The iconic tower is prominently featured in the background, covered with a light layer of snow, giving it a picturesque and serene appearance. The surrounding area includes leafless trees, their branches also dusted with snow, enhancing the wintery ambiance. The ground is blanketed in white, and the scene is framed by a classic Parisian lamp post and a fenced pathway. The overall setting exudes a calm, peaceful, and almost magical winter atmosphere in one of the world's most famous landmarks.\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])\n\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What's in this image?\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": \"https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg\"\n            }\n        ]\n    }\n]\n\n# Get the chat response\nstream_response = cli.chat.stream(\n    model='pixtral-large-2411',\n    messages=messages\n)\n\n\nfor chunk in stream_response:\n    print(chunk.data.choices[0].delta.content)\n\n\nThe\n image\n features\n a\n snow\ny\n scene\n of\n the\n E\niff\nel\n Tower\n in\n Paris\n,\n France\n.\n The\n icon\nic\n structure\n is\n seen\n in\n the\n background\n,\n tow\nering\n above\n the\n snow\n-\ncovered\n landscape\n.\n The\n fore\nground\n displays\n a\n pictures\nque\n path\nway\n lined\n with\n bare\n trees\n,\n their\n branches\n lightly\n dust\ned\n with\n snow\n.\n A\n classic\n Paris\nian\n l\nam\npp\nost\n is\n visible\n along\n the\n path\nway\n,\n adding\n to\n the\n winter\n charm\n.\n The\n overall\n atmosphere\n is\n ser\nene\n and\n magical\n,\n show\nc\nasing\n the\n beauty\n of\n Paris\n during\n the\n winter\n season\n.\n\n\n\n\n# Here are the list of different client methods:\n# - chat.complete (completion)\n# - chat.stream (completion streaming)\n# - chat.parse (structured output for instance)\n# - chat.fim.complete (fim: fill in middle / code generation)\n# - chat.ocr.process (ocr)\n# - chat.embeddings.create (embedding creation)\n\n\nm = {'role': 'user', 'content': \"I'm Jeremy\"}\nr = cli.chat.complete(messages = [m], model = model)\nr\n\nChatCompletionResponse(id='b6ed028d89724a50a70df00904a1bf93', object='chat.completion', model='mistral-large-2411', usage=UsageInfo(prompt_tokens=7, completion_tokens=27, total_tokens=34), created=1743585882, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Jeremy! Nice to meet you. How are you doing today? Is there something you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])\n\n\n\nprint(r)\n\nid='b6ed028d89724a50a70df00904a1bf93' object='chat.completion' model='mistral-large-2411' usage=UsageInfo(prompt_tokens=7, completion_tokens=27, total_tokens=34) created=1743585882 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Jeremy! Nice to meet you. How are you doing today? Is there something you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n\n\n\nFormatting output\n\n# Notes:\n#  - assistant message with prefix true, should be last message\n#  - assistant message with prefix false cannot be last.\n\n# Type of messages:\n#  - system: instructions for the assistant (system prompt I guess - sp)  (content, role='system')\n#  - user: user message (content, role='user')  \n#  - assistant: assistant message (content, tool_calls, prefix, role='assistant')\n#  - tool: tool call (content, tool_call_id, name, role='tool')\n\n# Check also:\n# - prefix\n# - safe_prompt (for guardrailing)\n\n\nsource\n\n\nfind_block\n\n find_block (r:collections.abc.Mapping)\n\nFind the message in r\n\n\n\n\nType\nDetails\n\n\n\n\nr\nMapping\nThe message to look in\n\n\n\n\n\nExported source\ndef find_block(r:abc.Mapping, # The message to look in\n              ):\n    \"Find the message in `r`\"\n    if isinstance(r, CompletionEvent): r = r.data # if async\n    m = nested_idx(r, 'choices', 0)\n    if not m: return m\n    if hasattr(m, 'message'): return m.message\n    return m.delta\n\n\n\nfind_block(r)\n\nAssistantMessage(content='Hello Jeremy! Nice to meet you. How are you doing today? Is there something you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant')\n\n\n\nsource\n\n\ncontents\n\n contents (r)\n\nHelper to get the contents from response r.\n\n\nExported source\ndef contents(r):\n    \"Helper to get the contents from response `r`.\"\n    blk = find_block(r)\n    if not blk: return r\n    if hasattr(blk, 'content'): return getattr(blk,'content')\n    return blk\n\n\n\ncontents(r)\n\n'Hello Jeremy! Nice to meet you. How are you doing today? Is there something you would like to talk about or do?'\n\n\n\n\nExported source\n@patch\ndef _repr_markdown_(self:ChatCompletionResponse):\n    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n    res = contents(self)\n    if not res: return f\"- {det}\"\n    return f\"\"\"{contents(self)}\n\n&lt;details&gt;\n\n- {det}\n\n&lt;/details&gt;\"\"\"\n\n\n\nr\n\nHello Jeremy! Nice to meet you. How are you doing today? Is there something you would like to talk about or do?\n\n\nid: b6ed028d89724a50a70df00904a1bf93\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=7 completion_tokens=27 total_tokens=34\ncreated: 1743585882\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘Hello Jeremy! Nice to meet you. How are you doing today? Is there something you would like to talk about or do?’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nr.usage\n\nUsageInfo(prompt_tokens=7, completion_tokens=27, total_tokens=34)\n\n\n\nsource\n\n\nusage\n\n usage (inp=0, out=0)\n\nSlightly more concise version of UsageInfo.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\nint\n0\ninput tokens\n\n\nout\nint\n0\nOutput tokens\n\n\n\n\n\nExported source\ndef usage(inp=0, # input tokens\n          out=0,  # Output tokens\n         ):\n    \"Slightly more concise version of `UsageInfo`.\"\n    return UsageInfo(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)\n\n\n\nusage(5)\n\nUsageInfo(prompt_tokens=5, completion_tokens=0, total_tokens=5)\n\n\n\nsource\n\n\nUsageInfo.__repr__\n\n UsageInfo.__repr__ ()\n\nReturn repr(self).\n\n\nExported source\n@patch\ndef __repr__(self:UsageInfo): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total_tokens}'\n\n\n\nr.usage\n\nIn: 7; Out: 27; Total: 34\n\n\n\nsource\n\n\nUsageInfo.__add__\n\n UsageInfo.__add__ (b)\n\nAdd together each of input_tokens and output_tokens\n\n\nExported source\n@patch\ndef __add__(self:UsageInfo, b):\n    \"Add together each of `input_tokens` and `output_tokens`\"\n    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)\n\n\n\nr.usage+r.usage\n\nIn: 14; Out: 54; Total: 68\n\n\n\n# Is it relevant to Mistral AI: TBD\ndef wrap_latex(text, md=True):\n    \"Replace MistralAI LaTeX codes with markdown-compatible ones\"\n    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", lambda o: f\"${o.group(1)}$\", text)\n    res = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", lambda o: f\"$${o.group(1)}$$\", text, flags=re.DOTALL)\n    if md: res = display.Markdown(res)\n    return res\n\n\nsource\n\n\nUsageInfo.total\n\n UsageInfo.total ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef total(self:UsageInfo): return self.total_tokens\n\n\n\nusage(5,1).total\n\n6\n\n\n\n\nCreating messages\nCreating message dictionaries manually can be tedious, so we’ll use helper functions from the msglm library.\nWe’ll use mk_msg to easily create messages like {'role': 'user', 'content': \"I'm Jeremy\"}. Since Mistral AI’s message format is compatible with OpenAI’s structure, we imported : from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n\nprompt = \"I'm Jeremy\"\nm = mk_msg(prompt)\nr = cli.chat.complete(messages=[m], model=model, max_tokens=100)\nr\n\nHello Jeremy! Nice to meet you. How can I assist you today? Let’s have a friendly conversation. 😊 How are you doing?\n\n\nid: defc0db8f5f94fb3a47f2fffeffacc3e\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=7 completion_tokens=31 total_tokens=38\ncreated: 1743585891\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=“Hello Jeremy! Nice to meet you. How can I assist you today? Let’s have a friendly conversation. 😊 How are you doing?”, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nWe can pass more than just text messages to Mistral AI. As we’ll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI.\nHere’s an example of a multimodal message containing text and images.\n{\n    'role': 'user', \n    'content': [\n        {'type': 'text', 'text': 'What is in the image?'},\n        {'type': 'image_url', 'image_url': {'url': f'data:{MEDIA_TYPE};base64,{IMG}'}}\n    ]\n}\nmk_msg infers the type automatically and creates the appropriate data structure.\nLLMs, don’t actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With Mistral AI, they always alternate user and assistant. We’ll use mk_msgs from msglm to make it easier to build up these dialog lists.\n\nmsgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \nmsgs\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How can I assist you today? Let's have a friendly conversation. 😊 How are you doing?\", tool_calls=None, prefix=False, role='assistant'),\n {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]\n\n\n\nr = cli.chat.complete(messages=msgs, model=model, max_tokens=100)\nr\n\nOf course! You just told me your name is Jeremy.\n\n\nid: 329522f02b7c499597ad0f72664ba439\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=51 completion_tokens=13 total_tokens=64\ncreated: 1743585893\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘Of course! You just told me your name is Jeremy.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nIn addition to the standard ‘user’ and ‘assistant’ roles found in the OpenAI API for instance, Mistral AI’s API also supports ‘system’ roles for providing instructions to the model and ‘tool’ roles for tool-based interactions.\nLet’s see it in action as demonstrated in Mistral AI’s guide on prefix use cases.\n\ninstruction = \"\"\"\nLet's roleplay.\nAlways give a single reply.\nRoleplay only, using dialogue only.\nDo not send any comments.\nDo not send any notes.\nDo not send any disclaimers.\n\"\"\"\n\nquestion = \"\"\"\nHi there!\n\"\"\"\n\nprefix = \"\"\"\nShakespeare: \n\"\"\"\n\nr = cli.chat.complete(\n    model=\"mistral-small-latest\",\n    messages=[\n        mk_msg(instruction, role=\"system\"),\n        mk_msg(question, role=\"user\"),\n        mk_msg(prefix, role=\"assistant\", prefix=True),\n    ],\n    max_tokens=128,\n)\nr\n\nShakespeare: Good morrow! Who art thou that greetest me so?\n\n\nid: ee991b548b83482ea1544aeaf5d60308\nobject: chat.completion\nmodel: mistral-small-latest\nusage: prompt_tokens=55 completion_tokens=19 total_tokens=74\ncreated: 1743585900\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘: morrow! Who art thou that greetest me so?’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]",
    "crumbs": [
      "Mistinguette's source"
    ]
  },
  {
    "objectID": "core.html#client",
    "href": "core.html#client",
    "title": "Mistinguette’s source",
    "section": "Client",
    "text": "Client\n\n# Note also the .fim (fill in middle) mistral method\n\n\nsource\n\nClient\n\n Client (model, cli=None)\n\nBasic LLM messages client.\n\n\nExported source\nclass Client:\n    def __init__(self, model, cli=None):\n        \"Basic LLM messages client.\"\n        self.model,self.use = model,usage(0,0)\n        self.text_only = model in text_only_models\n        self.can_use_tools = model in tool_models\n        self.c = (cli or Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\")))\n\n\n\nc = Client(model)\n\n\nc.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\n\nExported source\n@patch\ndef _r(self:Client, r:ChatCompletionResponse):\n    \"Store the result of the message and accrue total usage.\"\n    self.result = r\n    if getattr(r,'usage',None): self.use += r.usage\n    return r\n\n\n\nc._r(r)\nc.use\n\nIn: 55; Out: 19; Total: 74\n\n\n\nsource\n\n\nget_stream\n\n get_stream (r)\n\nNote that mistralai.Chat.complete and mistralai.Chat.stream have the same signature, we delegate to mistralai.Chat.complete below to avoid obfuscating **kwargs parameters as explained in fastcore documentation.\n\nsource\n\n\nClient.__call__\n\n Client.__call__ (msgs:list, sp:str='', maxtok=4096, stream:bool=False,\n                  temperature:OptionalNullable[float]=Unset(),\n                  top_p:Optional[float]=None,\n                  max_tokens:OptionalNullable[int]=Unset(),\n                  stop:Union[Stop,StopTypedDict,NoneType]=None,\n                  random_seed:OptionalNullable[int]=Unset(), response_form\n                  at:Union[mistralai.models.responseformat.ResponseFormat,\n                  mistralai.models.responseformat.ResponseFormatTypedDict,\n                  NoneType]=None, tools:OptionalNullable[typing.Union[typi\n                  ng.List[mistralai.models.tool.Tool],typing.List[mistrala\n                  i.models.tool.ToolTypedDict]]]=Unset(), tool_choice:Unio\n                  n[ChatCompletionRequestToolChoice,ChatCompletionRequestT\n                  oolChoiceTypedDict,NoneType]=None,\n                  presence_penalty:Optional[float]=None,\n                  frequency_penalty:Optional[float]=None,\n                  n:OptionalNullable[int]=Unset(), prediction:Union[mistra\n                  lai.models.prediction.Prediction,mistralai.models.predic\n                  tion.PredictionTypedDict,NoneType]=None,\n                  parallel_tool_calls:Optional[bool]=None,\n                  safe_prompt:Optional[bool]=None, retries:OptionalNullabl\n                  e[mistralai.utils.retries.RetryConfig]=Unset(),\n                  server_url:Optional[str]=None,\n                  timeout_ms:Optional[int]=None,\n                  http_headers:Optional[Mapping[str,str]]=None)\n\nMake a call to LLM.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nList of messages in the dialog\n\n\nsp\nstr\n\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntemperature\nOptionalNullable\n\n\n\n\ntop_p\nOptional\nNone\n\n\n\nmax_tokens\nOptionalNullable\n\n\n\n\nstop\nUnion\nNone\n\n\n\nrandom_seed\nOptionalNullable\n\n\n\n\nresponse_format\nUnion\nNone\n\n\n\ntools\nOptionalNullable\n\n\n\n\ntool_choice\nUnion\nNone\n\n\n\npresence_penalty\nOptional\nNone\n\n\n\nfrequency_penalty\nOptional\nNone\n\n\n\nn\nOptionalNullable\n\n\n\n\nprediction\nUnion\nNone\n\n\n\nparallel_tool_calls\nOptional\nNone\n\n\n\nsafe_prompt\nOptional\nNone\n\n\n\nretries\nOptionalNullable\n\n\n\n\nserver_url\nOptional\nNone\n\n\n\ntimeout_ms\nOptional\nNone\n\n\n\nhttp_headers\nOptional\nNone\n\n\n\n\n\n\nExported source\n@patch\n@delegates(mistralai.Chat.complete)\ndef __call__(self:Client,\n             msgs:list, # List of messages in the dialog\n             sp:str='', # System prompt\n             maxtok=4096, # Maximum tokens\n             stream:bool=False, # Stream response?\n             **kwargs):\n    \"Make a call to LLM.\"\n    if 'tools' in kwargs: assert self.can_use_tools, \"Tool use is not supported by the current model type.\"\n    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n    if sp and self.model in has_system_prompt_models: msgs = [mk_msg(sp, 'system')] + list(msgs)\n    chat_args = dict(model=self.model, messages=msgs, max_tokens=maxtok, **kwargs)\n    r = self.c.chat.stream(**chat_args) if stream else self.c.chat.complete(**chat_args)\n    return self._r(r) if not stream else get_stream(map(self._r, r))\n\n\n\nmsgs = [mk_msg('Hi')]\n\n\nc(msgs)\n\nHello! How can I assist you today? Let’s have a friendly conversation. 😊 How are you doing?\n\n\nid: c7803d4dbdf148b79b22933cc0ca36bf\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=4 completion_tokens=25 total_tokens=29\ncreated: 1743585912\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=“Hello! How can I assist you today? Let’s have a friendly conversation. 😊 How are you doing?”, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nc.use\n\nIn: 59; Out: 44; Total: 103\n\n\n\nfor o in c(msgs, stream=True): print(o, end='')\n\nHello! How can I assist you today? If you're up for it, I'd love to share a fun fact or a interesting topic to kick things off. How does that sound?\n\n\n\nc.use\n\nIn: 59; Out: 44; Total: 103",
    "crumbs": [
      "Mistinguette's source"
    ]
  },
  {
    "objectID": "core.html#tool-use",
    "href": "core.html#tool-use",
    "title": "Mistinguette’s source",
    "section": "Tool use",
    "text": "Tool use\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\n\nsource\n\nmk_mistralai_func\n\n mk_mistralai_func (f)\n\n\nsource\n\n\nmk_tool_choice\n\n mk_tool_choice (f)\n\n\nsysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n\n\nmk_mistralai_func(sums)\n\n{'type': 'function',\n 'function': {'name': 'sums',\n  'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer',\n     'description': 'First thing to sum'},\n    'b': {'type': 'integer', 'description': 'Second thing to sum'}},\n   'required': ['a', 'b']}}}\n\n\n\n[mk_mistralai_func(sums)]\n\n[{'type': 'function',\n  'function': {'name': 'sums',\n   'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer',\n      'description': 'First thing to sum'},\n     'b': {'type': 'integer', 'description': 'Second thing to sum'}},\n    'required': ['a', 'b']}}}]\n\n\n\nmk_tool_choice(\"sums\")\n\n{'type': 'function', 'function': {'name': 'sums'}}\n\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\ntools = [mk_mistralai_func(sums)]\ntool_choice = mk_tool_choice(\"sums\")\n\n\nmsgs = [mk_msg(pr)]\nr = c(msgs, sp=sysp, tools=tools)\nr\n\n\nid: 07d02cb9ef3844169736989a6f89f22f\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=144 completion_tokens=37 total_tokens=181\ncreated: 1743585922\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=’‘, tool_calls=[ToolCall(function=FunctionCall(name=’sums’, arguments=’{“a”: 604542, “b”: 6458932}’), id=’O3HmdZqak’, type=None, index=0)], prefix=False, role=’assistant’), finish_reason=‘tool_calls’)]\n\n\n\n\nr.choices[0]\n\nChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')\n\n\n\nm = find_block(r)\nm\n\nAssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)], prefix=False, role='assistant')\n\n\n\n# the assistant message contains a tool_calls field witht the list of tool available/passed\ntc = m.tool_calls\ntc\n\n[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)]\n\n\n\nfunc = tc[0].function\nfunc\n\nFunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}')\n\n\n\nsource\n\n\ncall_func_mistralai\n\n call_func_mistralai (func:mistralai.models.functioncall.FunctionCall,\n                      ns:Optional[collections.abc.Mapping]=None)\n\n\n\nExported source\ndef call_func_mistralai(func:FunctionCall, ns:Optional[abc.Mapping]=None): \n    return call_func(func.name, ast.literal_eval(func.arguments), ns)\n\n\n\nmk_ns(sums)\n\n{'sums': &lt;function __main__.sums(a: int, b: int) -&gt; int&gt;}\n\n\n\nns = mk_ns(sums)\nres = call_func_mistralai(func, ns=ns)\nres\n\nFinding the sum of 604542 and 6458932\n\n\n7063474\n\n\nCreating tool response messages with a structure compatible with OpenAI’s format:\n\nsource\n\n\nmk_toolres\n\n mk_toolres (r:collections.abc.Mapping,\n             ns:Optional[collections.abc.Mapping]=None, obj:Optional=None)\n\nCreate a tool_result message from response r.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nr\nMapping\n\nTool use request response\n\n\nns\nOptional\nNone\nNamespace to search for tools\n\n\nobj\nOptional\nNone\nClass to search for tools\n\n\n\n\n\nExported source\ndef mk_toolres(\n    r:abc.Mapping, # Tool use request response\n    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n    obj:Optional=None # Class to search for tools\n    ):\n    \"Create a `tool_result` message from response `r`.\"\n    r = mk_msg(r)\n    tcs = getattr(r, 'tool_calls', [])\n    res = [r]\n    if ns is None: ns = globals()\n    if obj is not None: ns = mk_ns(obj)\n    for tc in (tcs or []):\n        func = tc.function\n        cts = str(call_func_mistralai(func, ns=ns))\n        res.append(mk_msg(str(cts), 'tool', tool_call_id=tc.id, name=func.name))\n    return res\n\n\n\ntr = mk_toolres(r, ns=ns)\ntr\n\nFinding the sum of 604542 and 6458932\n\n\n[AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)], prefix=False, role='assistant'),\n {'role': 'tool',\n  'content': '7063474',\n  'tool_call_id': 'O3HmdZqak',\n  'name': 'sums'}]\n\n\n\ntr\n\n[AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)], prefix=False, role='assistant'),\n {'role': 'tool',\n  'content': '7063474',\n  'tool_call_id': 'O3HmdZqak',\n  'name': 'sums'}]\n\n\n\nmsgs\n\n[{'role': 'user', 'content': 'What is 604542+6458932?'}]\n\n\n\nmsgs += tr\n\n\nmsgs\n\n[{'role': 'user', 'content': 'What is 604542+6458932?'},\n AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)], prefix=False, role='assistant'),\n {'role': 'tool',\n  'content': '7063474',\n  'tool_call_id': 'O3HmdZqak',\n  'name': 'sums'}]\n\n\n\nmsgs\n\n[{'role': 'user', 'content': 'What is 604542+6458932?'},\n AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='O3HmdZqak', type=None, index=0)], prefix=False, role='assistant'),\n {'role': 'tool',\n  'content': '7063474',\n  'tool_call_id': 'O3HmdZqak',\n  'name': 'sums'}]\n\n\n\nres = c(msgs, sp=sysp, tools=tools)\nres\n\n604542+6458932=7063474\n\n\nid: 2190853f05504ec9bfa4e670a9112efa\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=211 completion_tokens=24 total_tokens=235\ncreated: 1743585932\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘604542+6458932=7063474’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nclass Dummy:\n    def sums(\n        self,\n        a:int,  # First thing to sum\n        b:int=1 # Second thing to sum\n    ) -&gt; int: # The sum of the inputs\n        \"Adds a + b.\"\n        print(f\"Finding the sum of {a} and {b}\")\n        return a + b\n\n\ntools = [mk_mistralai_func(Dummy.sums)]; tools\n\n[{'type': 'function',\n  'function': {'name': 'sums',\n   'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer',\n      'description': 'First thing to sum'},\n     'b': {'type': 'integer',\n      'description': 'Second thing to sum',\n      'default': 1}},\n    'required': ['a']}}}]\n\n\n\ntools = [mk_mistralai_func(Dummy.sums)]\n\no = Dummy()\nmsgs = mk_toolres(pr)\nr = c(msgs, sp=sysp, tools=tools)\n\nmsgs += mk_toolres(r, obj=o)\nres = c(msgs, sp=sysp, tools=tools)\nres\n\nFinding the sum of 604542 and 6458932\n\n\n604542 + 6458932 = 7063474\n\n\nid: babf6b7c41b347e9bcbc6fd0606a0756\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=213 completion_tokens=26 total_tokens=239\ncreated: 1743585936\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘604542 + 6458932 = 7063474’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nsource\n\n\nmock_tooluse\n\n mock_tooluse (name:str, res, **kwargs)\n\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nThe name of the called function\n\n\nres\n\nThe result of calling the function\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nExported source\ndef _mock_id(): return ''.join(choices(ascii_letters+digits, k=9))\n\ndef mock_tooluse(name:str, # The name of the called function\n                 res,  # The result of calling the function\n                 **kwargs): # The arguments to the function\n    \"\"\n    id = _mock_id()\n    func = dict(arguments=json.dumps(kwargs), name=name)\n    tc = dict(id=id, function=func, type='function')\n    req = dict(content=None, role='assistant', tool_calls=[tc])\n    resp = mk_msg('' if res is None else str(res), 'tool', tool_call_id=id, name=name)\n    return [req,resp]\n\n\nThis function mocks the messages needed to implement tool use, for situations where you want to insert tool use messages into a dialog without actually calling into the model.\n\ntu = mock_tooluse(name='sums', res=7063474, a=604542, b=6458932)\nr = c([mk_msg(pr)]+tu, tools=tools)\nr\n\n604542 + 6458932 = 7063474\n\n\nid: 19a68c8418e54030bb38be69d4e52a45\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=193 completion_tokens=26 total_tokens=219\ncreated: 1743585938\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘604542 + 6458932 = 7063474’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\nStructured outputs\n\nsource\n\n\nClient.structured\n\n Client.structured (msgs:list, tools:Optional[list]=None,\n                    obj:Optional=None,\n                    ns:Optional[collections.abc.Mapping]=None, sp:str='',\n                    maxtok=4096, stream:bool=False,\n                    temperature:OptionalNullable[float]=Unset(),\n                    top_p:Optional[float]=None,\n                    max_tokens:OptionalNullable[int]=Unset(),\n                    stop:Union[Stop,StopTypedDict,NoneType]=None,\n                    random_seed:OptionalNullable[int]=Unset(), response_fo\n                    rmat:Union[mistralai.models.responseformat.ResponseFor\n                    mat,mistralai.models.responseformat.ResponseFormatType\n                    dDict,NoneType]=None, tool_choice:Union[ChatCompletion\n                    RequestToolChoice,ChatCompletionRequestToolChoiceTyped\n                    Dict,NoneType]=None,\n                    presence_penalty:Optional[float]=None,\n                    frequency_penalty:Optional[float]=None,\n                    n:OptionalNullable[int]=Unset(), prediction:Union[mist\n                    ralai.models.prediction.Prediction,mistralai.models.pr\n                    ediction.PredictionTypedDict,NoneType]=None,\n                    parallel_tool_calls:Optional[bool]=None,\n                    safe_prompt:Optional[bool]=None, retries:OptionalNulla\n                    ble[mistralai.utils.retries.RetryConfig]=Unset(),\n                    server_url:Optional[str]=None,\n                    timeout_ms:Optional[int]=None,\n                    http_headers:Optional[Mapping[str,str]]=None)\n\nReturn the value of all tool calls (generally used for structured outputs)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nPrompt\n\n\ntools\nOptional\nNone\nList of tools to make available to OpenAI model\n\n\nobj\nOptional\nNone\nClass to search for tools\n\n\nns\nOptional\nNone\nNamespace to search for tools\n\n\nsp\nstr\n\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntemperature\nOptionalNullable\n\n\n\n\ntop_p\nOptional\nNone\n\n\n\nmax_tokens\nOptionalNullable\n\n\n\n\nstop\nUnion\nNone\n\n\n\nrandom_seed\nOptionalNullable\n\n\n\n\nresponse_format\nUnion\nNone\n\n\n\ntool_choice\nUnion\nNone\n\n\n\npresence_penalty\nOptional\nNone\n\n\n\nfrequency_penalty\nOptional\nNone\n\n\n\nn\nOptionalNullable\n\n\n\n\nprediction\nUnion\nNone\n\n\n\nparallel_tool_calls\nOptional\nNone\n\n\n\nsafe_prompt\nOptional\nNone\n\n\n\nretries\nOptionalNullable\n\n\n\n\nserver_url\nOptional\nNone\n\n\n\ntimeout_ms\nOptional\nNone\n\n\n\nhttp_headers\nOptional\nNone\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Client.__call__)\ndef structured(self:Client,\n               msgs: list, # Prompt\n               tools:Optional[list]=None, # List of tools to make available to OpenAI model\n               obj:Optional=None, # Class to search for tools\n               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n               **kwargs):\n    \"Return the value of all tool calls (generally used for structured outputs)\"\n    tools = listify(tools)\n    if ns is None: ns=mk_ns(*tools)\n    tools = [mk_mistralai_func(o) for o in tools]\n    if obj is not None: ns = mk_ns(obj)\n    res = self(msgs, tools=tools, tool_choice='required', **kwargs)\n    cts = getattr(res, 'choices', [])\n    tcs = [call_func_mistralai(t.function, ns=ns) for o in cts for t in (o.message.tool_calls or [])]\n    return tcs\n\n\nMistral’s API doesn’t natively support response formats [TO BE CONFIRMED], so we introduce a structured method to handle tool calling for this purpose. In this setup, the tool’s result is sent directly to the user without being passed back to the model.\n\nc.structured(mk_msgs(pr), tools=[sums])\n\nFinding the sum of 604542 and 6458932\n\n\n[7063474]",
    "crumbs": [
      "Mistinguette's source"
    ]
  },
  {
    "objectID": "core.html#chat",
    "href": "core.html#chat",
    "title": "Mistinguette’s source",
    "section": "Chat",
    "text": "Chat\n\nsource\n\nChat\n\n Chat (model:Optional[str]=None, cli:Optional[__main__.Client]=None,\n       sp='', tools:Optional[list]=None, tool_choice:Optional[str]=None)\n\nOpenAI chat client.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nOptional\nNone\nModel to use (leave empty if passing cli)\n\n\ncli\nOptional\nNone\nClient to use (leave empty if passing model)\n\n\nsp\nstr\n\nOptional system prompt\n\n\ntools\nOptional\nNone\nList of tools to make available\n\n\ntool_choice\nOptional\nNone\nForced tool choice\n\n\n\n\n\nExported source\nclass Chat:\n    def __init__(self,\n                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n                 sp='', # Optional system prompt\n                 tools:Optional[list]=None,  # List of tools to make available\n                 tool_choice:Optional[str]=None): # Forced tool choice\n        \"OpenAI chat client.\"\n        assert model or cli\n        self.c = (cli or Client(model))\n        self.h,self.sp,self.tools,self.tool_choice = [],sp,tools,tool_choice\n    \n    @property\n    def use(self): return self.c.use\n\n\n\nsp = \"Never mention what tools you use.\"\nchat = Chat(model, sp=sp)\nchat.c.use, chat.h\n\n(In: 0; Out: 0; Total: 0, [])\n\n\n\nsource\n\n\nChat.__call__\n\n Chat.__call__ (pr=None, stream:bool=False,\n                temperature:OptionalNullable[float]=Unset(),\n                top_p:Optional[float]=None,\n                max_tokens:OptionalNullable[int]=Unset(),\n                stop:Union[Stop,StopTypedDict,NoneType]=None,\n                random_seed:OptionalNullable[int]=Unset(), response_format\n                :Union[mistralai.models.responseformat.ResponseFormat,mist\n                ralai.models.responseformat.ResponseFormatTypedDict,NoneTy\n                pe]=None, tools:OptionalNullable[typing.Union[typing.List[\n                mistralai.models.tool.Tool],typing.List[mistralai.models.t\n                ool.ToolTypedDict]]]=Unset(), tool_choice:Union[ChatComple\n                tionRequestToolChoice,ChatCompletionRequestToolChoiceTyped\n                Dict,NoneType]=None,\n                presence_penalty:Optional[float]=None,\n                frequency_penalty:Optional[float]=None,\n                n:OptionalNullable[int]=Unset(), prediction:Union[mistrala\n                i.models.prediction.Prediction,mistralai.models.prediction\n                .PredictionTypedDict,NoneType]=None,\n                parallel_tool_calls:Optional[bool]=None,\n                safe_prompt:Optional[bool]=None, retries:OptionalNullable[\n                mistralai.utils.retries.RetryConfig]=Unset(),\n                server_url:Optional[str]=None,\n                timeout_ms:Optional[int]=None,\n                http_headers:Optional[Mapping[str,str]]=None)\n\nAdd prompt pr to dialog and get a response\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\nNoneType\nNone\nPrompt / message\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntemperature\nOptionalNullable\n\n\n\n\ntop_p\nOptional\nNone\n\n\n\nmax_tokens\nOptionalNullable\n\n\n\n\nstop\nUnion\nNone\n\n\n\nrandom_seed\nOptionalNullable\n\n\n\n\nresponse_format\nUnion\nNone\n\n\n\ntools\nOptionalNullable\n\n\n\n\ntool_choice\nUnion\nNone\n\n\n\npresence_penalty\nOptional\nNone\n\n\n\nfrequency_penalty\nOptional\nNone\n\n\n\nn\nOptionalNullable\n\n\n\n\nprediction\nUnion\nNone\n\n\n\nparallel_tool_calls\nOptional\nNone\n\n\n\nsafe_prompt\nOptional\nNone\n\n\n\nretries\nOptionalNullable\n\n\n\n\nserver_url\nOptional\nNone\n\n\n\ntimeout_ms\nOptional\nNone\n\n\n\nhttp_headers\nOptional\nNone\n\n\n\n\n\n\nExported source\n@patch\n@delegates(mistralai.Chat.complete)\ndef __call__(self:Chat,\n             pr=None,  # Prompt / message\n             stream:bool=False, # Stream response?\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response\"\n    if isinstance(pr,str): pr = pr.strip()\n    if pr: self.h.append(mk_msg(pr))\n    if self.tools: kwargs['tools'] = [mk_mistralai_func(o) for o in self.tools]\n    if self.tool_choice: kwargs['tool_choice'] = mk_tool_choice(self.tool_choice)\n    res = self.c(self.h, sp=self.sp, stream=stream, **kwargs)\n    self.h += mk_toolres(res, ns=self.tools)\n    return res\n\n\n\nchat(\"I'm Jeremy\")\nchat(\"What's my name?\")\n\nYou mentioned that your name is Jeremy! How can I assist you today, Jeremy?\n\n\nid: 8ffe24005682455eb3a88b18eeedc8cd\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=38 completion_tokens=18 total_tokens=56\ncreated: 1743585946\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘You mentioned that your name is Jeremy! How can I assist you today, Jeremy?’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\nchat.h\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n AssistantMessage(content='Hello Jeremy! Nice to meet you. How are you today?', tool_calls=None, prefix=False, role='assistant'),\n {'role': 'user', 'content': \"What's my name?\"},\n AssistantMessage(content='You mentioned that your name is Jeremy! How can I assist you today, Jeremy?', tool_calls=None, prefix=False, role='assistant')]\n\n\n\nchat = Chat(model, sp=sp)\nfor o in chat(\"I'm Jeremy\", stream=True):\n    o = contents(o)\n    if o and isinstance(o, str): print(o, end='')\n\nHello Jeremy! Nice to meet you. How are you today?\n\n\n\nchat = Chat(model, sp=sp)\nproblem = \"1233 * 4297\"\nprint(f\"Correct Answer: {problem} = {eval(problem)}\")\n\nr = chat(f\"what is {problem}?\")\nprint(f'Model answer: {contents(r)}')\n\nCorrect Answer: 1233 * 4297 = 5298201\nModel answer: The product of 1233 and 4297 is 5,297,901.\n\n\n\n\nChat tool use\n\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\n\nmodel\n\n'mistral-large-2411'\n\n\n\n## Debug\n\n\nsp\n\n'Never mention what tools you use.'\n\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\n\nchat = Chat(model, sp=sp, tools=[sums])\n\n\nr = chat(pr)\nr\n\nFinding the sum of 604542 and 6458932\n\n\n\nid: 9d017b8f4278452787f840108651a697\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=130 completion_tokens=37 total_tokens=167\ncreated: 1743589168\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=’‘, tool_calls=[ToolCall(function=FunctionCall(name=’sums’, arguments=’{“a”: 604542, “b”: 6458932}’), id=’qCyqNX6qy’, type=None, index=0)], prefix=False, role=’assistant’), finish_reason=‘tool_calls’)]\n\n\n\n\nchat()\n\nWhat is 604542+6458932?\n604542+6458932=7063474\n\n\nid: 6fc66b906dab4fbd91e81bf08f47f375\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=201 completion_tokens=43 total_tokens=244\ncreated: 1743589186\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘What is 604542+6458932?+6458932=7063474’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]\n\n\n\n\n\n## Debug\n\n\nchat = Chat(model, sp=sp, tools=[sums])\nr = chat(pr)\nr\n\nFinding the sum of 604542 and 6458932\n\n\n\nid: c97386aa2f884c04ab53f17aa074a7c3\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=130 completion_tokens=37 total_tokens=167\ncreated: 1743589105\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=’‘, tool_calls=[ToolCall(function=FunctionCall(name=’sums’, arguments=’{“a”: 604542, “b”: 6458932}’), id=’hvktCFryO’, type=None, index=0)], prefix=False, role=’assistant’), finish_reason=‘tool_calls’)]\n\n\n\n\nchat()\n\n604542 + 6458932 equals 7063474. Is there anything else I can help you with?\n\n\nid: 65451e31882846b8b42fd87de56702f1\nobject: chat.completion\nmodel: mistral-large-2411\nusage: prompt_tokens=195 completion_tokens=37 total_tokens=232\ncreated: 1743589109\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘604542 + 6458932 equals 7063474. Is there anything else I can help you with?’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]",
    "crumbs": [
      "Mistinguette's source"
    ]
  },
  {
    "objectID": "core.html#images",
    "href": "core.html#images",
    "title": "Mistinguette’s source",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\n# Image is Cute_dog.jpg from Wikimedia\nfn = Path('samples/puppy.jpg')\ndisplay.Image(filename=fn, width=200)\n\n\n\n\n\n\n\n\n\nimg = fn.read_bytes()\n\nMistral AI expects an image message to have the following structure\n{\n  \"type\": \"image_url\",\n  \"image_url\": {\n    \"url\": f\"data:{MEDIA_TYPE};base64,{IMG}\"\n  }\n}\nmsglm automatically detects if a message is an image, encodes it, and generates the data structure above. All we need to do is a create a list containing our image and a query and then pass it to mk_msg.\nLet’s try it out…\n\nq = \"In brief, what color flowers are in this image?\"\nmsg = [mk_msg(img), mk_msg(q)]\n\n\nvision_models\n\n['pixtral-large-2411', 'mistral-small-2503', 'mistral-ocr-2503']\n\n\n\nc = Chat(vision_models[0])\nc([img, q])\n\nThe flowers in the image are purple with yellow centers.\n\n\nid: 7419ce04d287433991f72ed2b079b1bc\nobject: chat.completion\nmodel: pixtral-large-2411\nusage: prompt_tokens=274 completion_tokens=11 total_tokens=285\ncreated: 1743582905\nchoices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=‘The flowers in the image are purple with yellow centers.’, tool_calls=None, prefix=False, role=‘assistant’), finish_reason=‘stop’)]",
    "crumbs": [
      "Mistinguette's source"
    ]
  }
]
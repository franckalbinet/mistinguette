{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistinguette's source "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from collections import abc\n",
    "try: from IPython import display\n",
    "except: display=None\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "import ast, json, base64\n",
    "from random import choices\n",
    "from string import ascii_letters,digits\n",
    "\n",
    "# from rich import print\n",
    "from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n",
    "from toolslm.funccall import *\n",
    "\n",
    "import mistralai\n",
    "from mistralai import Mistral\n",
    "from mistralai.models import ChatCompletionChoice, ChatCompletionResponse, UsageInfo, CompletionEvent\n",
    "from mistralai.models.functioncall import FunctionCall\n",
    "from mistralai.types import BaseModel\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/functioncall.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "model_types = {\n",
    "    # Premier models\n",
    "    'codestral-2501': 'codestral-latest', # code generation model\n",
    "    'mistral-large-2411': 'mistral-large-latest', # top-tier reasoning model for high-complexity tasks\n",
    "    'pixtral-large-2411': 'pixtral-large-latest', # frontier-class multimodal model\n",
    "    'mistral-saba-2502': 'mistral-saba-latest', # model for languages from the Middle East and South Asia\n",
    "    'ministral-3b-2410': 'ministral-3b-latest', # edge model\n",
    "    'ministral-8b-2410': 'ministral-8b-latest', # edge model with high performance/price ratio\n",
    "    'mistral-embed-2312': 'mistral-embed', # embedding model\n",
    "    'mistral-moderation-2411': 'mistral-moderation-latest', # moderation service to detect harmful text content\n",
    "    'mistral-ocr-2503': 'mistral-ocr-latest', # OCR model to extract interleaved text and images\n",
    "    \n",
    "    # Free models (with weight availability)\n",
    "    'mistral-small-2503': 'mistral-small-latest', # small model with image understanding capabilities\n",
    "    \n",
    "    # Research models\n",
    "    'open-mistral-nemo-2407': 'open-mistral-nemo', # multilingual open source model\n",
    "}\n",
    "\n",
    "all_models = list(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "vision_models = ['pixtral-large-2411', 'mistral-small-2503', 'mistral-ocr-2503']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "embed_models = ['mistral-embed-2312']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "ocr_models = ['mistral-ocr-2503']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "tool_models = ['codestral-2501', 'mistral-large-2411', \n",
    "               'mistral-small-2503', 'ministral-3b-2410', 'ministral-8b-2410',\n",
    "               'pixtral-large-2411', 'open-mistral-nemo-2407']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "text_only_models = set(all_models) - set(vision_models) - set(embed_models) - set(ocr_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "has_streaming_models = set(all_models) - set(embed_models) - set(ocr_models)\n",
    "has_system_prompt_models = set(all_models) - set(embed_models) - set(ocr_models)\n",
    "has_temperature_models = set(all_models) - set(embed_models) - set(ocr_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all models except codestral-mamba support custom structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "models = all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral-large-2411'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[1]; model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Mistral(api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what Mistral's SDK provides for interacting with Python. To use it, pass it a list of *messages*, with *content* and a *role*. The roles should alternate between *user* and *assistant*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"What's in this image?\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": \"https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the chat response\n",
    "chat_response = cli.chat.complete(\n",
    "    model='pixtral-large-2411',\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image depicts a picturesque winter scene at the Eiffel Tower in Paris, France. The iconic tower stands prominently in the background, partially obscured by the snowfall and haze, which creates a serene and almost dreamlike atmosphere. The foreground features a snow-covered pathway lined with bare trees, their branches laden with snow. A traditional Parisian street lamp and a fence run alongside the path, enhancing the charm of the scene. The snow blankets the ground and trees, adding to the tranquil and magical ambiance of the winter landscape. The overall mood of the image is calm and peaceful, capturing the beauty of Paris in the wintertime.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 95e0877bb7bc4c85afb5152b7f795b37\n",
       "- object: chat.completion\n",
       "- model: pixtral-large-2411\n",
       "- usage: prompt_tokens=1360 completion_tokens=144 total_tokens=1504\n",
       "- created: 1743493942\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='The image depicts a picturesque winter scene at the Eiffel Tower in Paris, France. The iconic tower stands prominently in the background, partially obscured by the snowfall and haze, which creates a serene and almost dreamlike atmosphere. The foreground features a snow-covered pathway lined with bare trees, their branches laden with snow. A traditional Parisian street lamp and a fence run alongside the path, enhancing the charm of the scene. The snow blankets the ground and trees, adding to the tranquil and magical ambiance of the winter landscape. The overall mood of the image is calm and peaceful, capturing the beauty of Paris in the wintertime.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='95e0877bb7bc4c85afb5152b7f795b37', object='chat.completion', model='pixtral-large-2411', usage=In: 1360; Out: 144; Total: 1504, created=1743493942, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='The image depicts a picturesque winter scene at the Eiffel Tower in Paris, France. The iconic tower stands prominently in the background, partially obscured by the snowfall and haze, which creates a serene and almost dreamlike atmosphere. The foreground features a snow-covered pathway lined with bare trees, their branches laden with snow. A traditional Parisian street lamp and a fence run alongside the path, enhancing the charm of the scene. The snow blankets the ground and trees, adding to the tranquil and magical ambiance of the winter landscape. The overall mood of the image is calm and peaceful, capturing the beauty of Paris in the wintertime.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"What's in this image?\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": \"https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the chat response\n",
    "stream_response = cli.chat.stream(\n",
    "    model='pixtral-large-2411',\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The\n",
      " image\n",
      " dep\n",
      "icts\n",
      " a\n",
      " ser\n",
      "ene\n",
      " winter\n",
      " scene\n",
      " in\n",
      " which\n",
      " a\n",
      " snow\n",
      "-\n",
      "covered\n",
      " path\n",
      "way\n",
      " leads\n",
      " towards\n",
      " the\n",
      " icon\n",
      "ic\n",
      " E\n",
      "iff\n",
      "el\n",
      " Tower\n",
      " in\n",
      " Paris\n",
      ",\n",
      " France\n",
      ".\n",
      " The\n",
      " trees\n",
      " l\n",
      "ining\n",
      " the\n",
      " path\n",
      " are\n",
      " drap\n",
      "ed\n",
      " in\n",
      " snow\n",
      ",\n",
      " and\n",
      " the\n",
      " entire\n",
      " environment\n",
      " has\n",
      " a\n",
      " peaceful\n",
      ",\n",
      " tr\n",
      "anqu\n",
      "il\n",
      " atmosphere\n",
      ".\n",
      " The\n",
      " E\n",
      "iff\n",
      "el\n",
      " Tower\n",
      " itself\n",
      " stands\n",
      " tall\n",
      " and\n",
      " maj\n",
      "estic\n",
      ",\n",
      " partially\n",
      " covered\n",
      " with\n",
      " snow\n",
      ",\n",
      " adding\n",
      " to\n",
      " the\n",
      " beauty\n",
      " of\n",
      " the\n",
      " winter\n",
      "y\n",
      " landscape\n",
      ".\n",
      " A\n",
      " classic\n",
      " street\n",
      " lamp\n",
      " is\n",
      " visible\n",
      " along\n",
      " the\n",
      " path\n",
      ",\n",
      " enh\n",
      "ancing\n",
      " the\n",
      " pictures\n",
      "que\n",
      " and\n",
      " tim\n",
      "eless\n",
      " quality\n",
      " of\n",
      " the\n",
      " scene\n",
      ".\n",
      " The\n",
      " sky\n",
      " is\n",
      " over\n",
      "cast\n",
      ",\n",
      " contributing\n",
      " to\n",
      " the\n",
      " calm\n",
      ",\n",
      " mut\n",
      "ed\n",
      " t\n",
      "ones\n",
      " of\n",
      " the\n",
      " photograph\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in stream_response:\n",
    "    print(chunk.data.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the list of different client methods:\n",
    "# - chat.complete (completion)\n",
    "# - chat.stream (completion streaming)\n",
    "# - chat.parse (structured output for instance)\n",
    "# - chat.fim.complete (fim: fill in middle / code generation)\n",
    "# - chat.ocr.process (ocr)\n",
    "# - chat.embeddings.create (embedding creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 9334bba8acc544bfae203558433e5a31\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=7 completion_tokens=37 total_tokens=44\n",
       "- created: 1743493952\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='9334bba8acc544bfae203558433e5a31', object='chat.completion', model='mistral-large-2411', usage=In: 7; Out: 37; Total: 44, created=1743493952, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {'role': 'user', 'content': \"I'm Jeremy\"}\n",
    "r = cli.chat.complete(messages = [m], model = model)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='9334bba8acc544bfae203558433e5a31' object='chat.completion' model='mistral-large-2411' usage=In: 7; Out: 37; Total: 44 created=1743493952 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def find_block(r:abc.Mapping, # The message to look in\n",
    "              ):\n",
    "    \"Find the message in `r`\"\n",
    "    if isinstance(r, CompletionEvent): r = r.data # if async\n",
    "    m = nested_idx(r, 'choices', 0)\n",
    "    if not m: return m\n",
    "    if hasattr(m, 'message'): return m.message\n",
    "    return m.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\", tool_calls=None, prefix=False, role='assistant')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_block(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    if hasattr(blk, 'content'): return getattr(blk,'content')\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:ChatCompletionResponse):\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n",
    "    res = contents(self)\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{contents(self)}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 9334bba8acc544bfae203558433e5a31\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=7 completion_tokens=37 total_tokens=44\n",
       "- created: 1743493952\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='9334bba8acc544bfae203558433e5a31', object='chat.completion', model='mistral-large-2411', usage=In: 7; Out: 37; Total: 44, created=1743493952, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Jeremy! Nice to meet you. How are you doing today? Let's have a friendly conversation. How about I share an interesting fact or a light joke to start?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 7; Out: 37; Total: 44"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # input tokens\n",
    "          out=0,  # Output tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `UsageInfo`.\"\n",
    "    return UsageInfo(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 0; Total: 5"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:UsageInfo): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 7; Out: 37; Total: 44"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:UsageInfo, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 14; Out: 74; Total: 88"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage+r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it relevant to Mistral AI: TBD\n",
    "def wrap_latex(text, md=True):\n",
    "    \"Replace MistralAI LaTeX codes with markdown-compatible ones\"\n",
    "    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", lambda o: f\"${o.group(1)}$\", text)\n",
    "    res = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", lambda o: f\"$${o.group(1)}$$\", text, flags=re.DOTALL)\n",
    "    if md: res = display.Markdown(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def total(self:UsageInfo): return self.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5,1).total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating message dictionaries manually can be tedious, so we'll use helper functions from the `msglm` library.\n",
    " \n",
    "We'll use `mk_msg` to easily create messages like `{'role': 'user', 'content': \"I'm Franck\"}`. Since Mistral AI's message format is compatible with OpenAI's structure, we imported : `from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: eafcf22a35514faaba12cf8075ff4c51\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=8 completion_tokens=28 total_tokens=36\n",
       "- created: 1743493954\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='eafcf22a35514faaba12cf8075ff4c51', object='chat.completion', model='mistral-large-2411', usage=In: 8; Out: 28; Total: 36, created=1743493954, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Franck\"\n",
    "m = mk_msg(prompt)\n",
    "r = cli.chat.complete(messages=[m], model=model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass more than just text messages to Mistral AI. As we'll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI. \n",
    "\n",
    "Here's an example of a multimodal message containing text and images. \n",
    "\n",
    "```json\n",
    "{\n",
    "    'role': 'user', \n",
    "    'content': [\n",
    "        {'type': 'text', 'text': 'What is in the image?'},\n",
    "        {'type': 'image_url', 'image_url': {'url': f'data:{MEDIA_TYPE};base64,{IMG}'}}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "`mk_msg` infers the type automatically and creates the appropriate data structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs, don't actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With Mistral AI, they always alternate *user* and *assistant*. We'll use `mk_msgs` from `msglm` to make it easier to build up these dialog lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Franck\"},\n",
       " AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'),\n",
       " {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! You just told me your name is Franck.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 99d1892899934ddd94c899f71d3ffcb3\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=49 completion_tokens=14 total_tokens=63\n",
       "- created: 1743493955\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='Of course! You just told me your name is Franck.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='99d1892899934ddd94c899f71d3ffcb3', object='chat.completion', model='mistral-large-2411', usage=In: 49; Out: 14; Total: 63, created=1743493955, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Of course! You just told me your name is Franck.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.chat.complete(messages=msgs, model=model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the standard 'user' and 'assistant' roles found in the OpenAI API for instance, Mistral AI's API also supports 'system' roles for providing instructions to the model and 'tool' roles for tool-based interactions. \n",
    "\n",
    "Let's see it in action as demonstrated in [Mistral AI's guide](https://docs.mistral.ai/guides/prefix/) on prefix use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Shakespeare: \n",
       "Good morrow! Who art thou that dost greet me so?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: a51b7622c9ef46dcb940ffc098bc7eab\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=55 completion_tokens=19 total_tokens=74\n",
       "- created: 1743493968\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='\\nShakespeare: \\nGood morrow! Who art thou that dost greet me so?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='a51b7622c9ef46dcb940ffc098bc7eab', object='chat.completion', model='mistral-small-latest', usage=In: 55; Out: 19; Total: 74, created=1743493968, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='\\nShakespeare: \\nGood morrow! Who art thou that dost greet me so?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"\"\"\n",
    "Let's roleplay.\n",
    "Always give a single reply.\n",
    "Roleplay only, using dialogue only.\n",
    "Do not send any comments.\n",
    "Do not send any notes.\n",
    "Do not send any disclaimers.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Hi there!\n",
    "\"\"\"\n",
    "\n",
    "prefix = \"\"\"\n",
    "Shakespeare: \n",
    "\"\"\"\n",
    "\n",
    "r = cli.chat.complete(\n",
    "    model=\"mistral-small-latest\",\n",
    "    messages=[\n",
    "        mk_msg(instruction, role=\"system\"),\n",
    "        mk_msg(question, role=\"user\"),\n",
    "        mk_msg(prefix, role=\"assistant\", prefix=True),\n",
    "    ],\n",
    "    max_tokens=128,\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note also the .fim (fill in middle) mistral method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        self.can_use_tools = model in tool_models\n",
    "        self.c = (cli or Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(\"mistral-small-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 0; Out: 0; Total: 0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _r(self:Client, r:ChatCompletionResponse):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self.result = r\n",
    "    if getattr(r,'usage',None): self.use += r.usage\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 55; Out: 19; Total: 74"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stream(r):\n",
    "    for o in r:\n",
    "        o = contents(o)\n",
    "        if o and isinstance(o, str): yield(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `mistralai.Chat.complete` and `mistralai.Chat.stream` have the same signature, we **delegate** to `mistralai.Chat.complete` below to avoid obfuscating `**kwargs` parameters as explained in [fastcore documentation](https://fastcore.fast.ai/meta.html#delegates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(mistralai.Chat.complete)\n",
    "def __call__(self:Client,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp:str='', # System prompt\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream:bool=False, # Stream response?\n",
    "             **kwargs):\n",
    "    \"Make a call to LLM.\"\n",
    "    if 'tools' in kwargs: assert not self.can_use_tools, \"Tool use is not supported by the current model type.\"\n",
    "    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n",
    "    if sp and self.model in has_system_prompt_models: msgs = [mk_msg(sp, 'system')] + list(msgs)\n",
    "    chat_args = dict(model=self.model, messages=msgs, max_tokens=maxtok, **kwargs)\n",
    "    r = self.c.chat.stream(**chat_args) if stream else self.c.chat.complete(**chat_args)\n",
    "    return self._r(r) if not stream else get_stream(map(self._r, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [mk_msg('Hi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today? Let's chat about anything you'd like. ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: d1ee57f57ebb432ca25bbea8ebc0e37f\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=4 completion_tokens=21 total_tokens=25\n",
       "- created: 1743493981\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello! How can I assist you today? Let's chat about anything you'd like. ðŸ˜Š\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='d1ee57f57ebb432ca25bbea8ebc0e37f', object='chat.completion', model='mistral-small-latest', usage=In: 4; Out: 21; Total: 25, created=1743493981, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello! How can I assist you today? Let's chat about anything you'd like. ðŸ˜Š\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 59; Out: 40; Total: 99"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? Let's chat about anything you'd like. ðŸ˜Š"
     ]
    }
   ],
   "source": [
    "for o in c(msgs, stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 59; Out: 40; Total: 99"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_mistralai_func(f): \n",
    "    sc = get_schema(f, 'parameters')\n",
    "    sc['parameters'].pop('title', None)\n",
    "    return dict(type='function', function=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tool_choice(f): return dict(type='function', function={'name':f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'sums',\n",
       "  'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer',\n",
       "     'description': 'First thing to sum'},\n",
       "    'b': {'type': 'integer', 'description': 'Second thing to sum'}},\n",
       "   'required': ['a', 'b']}}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_mistralai_func(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'sums',\n",
       "   'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer',\n",
       "      'description': 'First thing to sum'},\n",
       "     'b': {'type': 'integer', 'description': 'Second thing to sum'}},\n",
       "    'required': ['a', 'b']}}}]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mk_mistralai_func(sums)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function', 'function': {'name': 'sums'}}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_tool_choice(\"sums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\"\n",
    "tools = [mk_mistralai_func(sums)]\n",
    "tool_choice = mk_tool_choice(\"sums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- id: daf3e056fb8a43e49d723cb6c3436fa0\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=120 completion_tokens=36 total_tokens=156\n",
       "- created: 1743497557\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')]"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='daf3e056fb8a43e49d723cb6c3436fa0', object='chat.completion', model='mistral-small-latest', usage=In: 120; Out: 36; Total: 156, created=1743497557, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [mk_msg(pr)]\n",
    "r = c(msgs, sp=sysp, tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionChoice(index=0, message=AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'), finish_reason='tool_calls')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = find_block(r)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the assistant message contains a tool_calls field witht the list of tool available/passed\n",
    "tc = m.tool_calls\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI returns \"Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums')\"\n",
    "func = tc[0].function\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def call_func_mistralai(func:FunctionCall, ns:Optional[abc.Mapping]=None): \n",
    "    return call_func(func.name, ast.literal_eval(func.arguments), ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sums': <function __main__.sums(a: int, b: int) -> int>}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_ns(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7063474"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = mk_ns(sums)\n",
    "res = call_func_mistralai(func, ns=ns)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tool response messages with a structure compatible with OpenAI's format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_toolres(\n",
    "    r:abc.Mapping, # Tool use request response\n",
    "    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "    obj:Optional=None # Class to search for tools\n",
    "    ):\n",
    "    \"Create a `tool_result` message from response `r`.\"\n",
    "    r = mk_msg(r)\n",
    "    tcs = getattr(r, 'tool_calls', [])\n",
    "    res = [r]\n",
    "    if ns is None: ns = globals()\n",
    "    if obj is not None: ns = mk_ns(obj)\n",
    "    for tc in (tcs or []):\n",
    "        func = tc.function\n",
    "        cts = str(call_func_mistralai(func, ns=ns))\n",
    "        res.append(mk_msg(str(cts), 'tool', tool_call_id=tc.id, name=func.name))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'),\n",
       " {'role': 'tool',\n",
       "  'content': '7063474',\n",
       "  'tool_call_id': 'Fka3v3Tba',\n",
       "  'name': 'sums'}]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=ns)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'),\n",
       " {'role': 'tool',\n",
       "  'content': '7063474',\n",
       "  'tool_call_id': 'Fka3v3Tba',\n",
       "  'name': 'sums'}]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 604542+6458932?'}]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs += tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 604542+6458932?'},\n",
       " AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'),\n",
       " {'role': 'tool',\n",
       "  'content': '7063474',\n",
       "  'tool_call_id': 'Fka3v3Tba',\n",
       "  'name': 'sums'}]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 604542+6458932?'},\n",
       " AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='sums', arguments='{\"a\": 604542, \"b\": 6458932}'), id='Fka3v3Tba', type=None, index=0)], prefix=False, role='assistant'),\n",
       " {'role': 'tool',\n",
       "  'content': '7063474',\n",
       "  'tool_call_id': 'Fka3v3Tba',\n",
       "  'name': 'sums'}]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7063474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 9f7a62fc43e94e15ba999a2c4b9dd520\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=186 completion_tokens=29 total_tokens=215\n",
       "- created: 1743497572\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='The sum of 604542 and 6458932 is 7063474.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='9f7a62fc43e94e15ba999a2c4b9dd520', object='chat.completion', model='mistral-small-latest', usage=In: 186; Out: 29; Total: 215, created=1743497572, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='The sum of 604542 and 6458932 is 7063474.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(msgs, sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy:\n",
    "    def sums(\n",
    "        self,\n",
    "        a:int,  # First thing to sum\n",
    "        b:int=1 # Second thing to sum\n",
    "    ) -> int: # The sum of the inputs\n",
    "        \"Adds a + b.\"\n",
    "        print(f\"Finding the sum of {a} and {b}\")\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'sums',\n",
       "   'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer',\n",
       "      'description': 'First thing to sum'},\n",
       "     'b': {'type': 'integer',\n",
       "      'description': 'Second thing to sum',\n",
       "      'default': 1}},\n",
       "    'required': ['a']}}}]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [mk_mistralai_func(Dummy.sums)]; tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7063474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 5ed6ca8651194b2096dca7573d325260\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=190 completion_tokens=29 total_tokens=219\n",
       "- created: 1743498164\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='The sum of 604542 and 6458932 is 7063474.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='5ed6ca8651194b2096dca7573d325260', object='chat.completion', model='mistral-small-latest', usage=In: 190; Out: 29; Total: 219, created=1743498164, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='The sum of 604542 and 6458932 is 7063474.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [mk_mistralai_func(Dummy.sums)]\n",
    "\n",
    "o = Dummy()\n",
    "msgs = mk_toolres(pr)\n",
    "r = c(msgs, sp=sysp, tools=tools)\n",
    "\n",
    "msgs += mk_toolres(r, obj=o)\n",
    "res = c(msgs, sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _mock_id(): return ''.join(choices(ascii_letters+digits, k=9))\n",
    "\n",
    "def mock_tooluse(name:str, # The name of the called function\n",
    "                 res,  # The result of calling the function\n",
    "                 **kwargs): # The arguments to the function\n",
    "    \"\"\n",
    "    id = _mock_id()\n",
    "    func = dict(arguments=json.dumps(kwargs), name=name)\n",
    "    tc = dict(id=id, function=func, type='function')\n",
    "    req = dict(content=None, role='assistant', tool_calls=[tc])\n",
    "    resp = mk_msg('' if res is None else str(res), 'tool', tool_call_id=id, name=name)\n",
    "    return [req,resp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function mocks the messages needed to implement tool use, for situations where you want to insert tool use messages into a dialog without actually calling into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7063474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: bd79c58569b7455b9602491a09262548\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=188 completion_tokens=29 total_tokens=217\n",
       "- created: 1743498684\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='The sum of 604542 and 6458932 is 7063474.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='bd79c58569b7455b9602491a09262548', object='chat.completion', model='mistral-small-latest', usage=In: 188; Out: 29; Total: 217, created=1743498684, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='The sum of 604542 and 6458932 is 7063474.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu = mock_tooluse(name='sums', res=7063474, a=604542, b=6458932)\n",
    "r = c([mk_msg(pr)]+tu, tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "#  - assistant message with prefix true, should be last message\n",
    "#  - assistant message with prefix false cannot be last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of messages:\n",
    "#  - system: instructions for the assistant (system prompt I guess - sp)  (content, role='system')\n",
    "#  - user: user message (content, role='user')  \n",
    "#  - assistant: assistant message (content, tool_calls, prefix, role='assistant')\n",
    "#  - tool: tool call (content, tool_call_id, name, role='tool')\n",
    "\n",
    "# Check also:\n",
    "# - prefix\n",
    "# - safe_prompt (for guardrailing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [\n",
    "    {'role': 'system', 'content': \"You are a helpful assistant full of irony\"},\n",
    "    {'role': 'user', 'content': \"I'm Franck\"},\n",
    "    {'role': 'assistant', 'content': \"Well, Franck, it's a pleasure to meet you. I must say, I've always been a fan of the name. It's strong, it's classic, it's... frankly, it's fantastic. You've set a high bar for yourself, Franck. Let's hope you can live up to the grandeur of your name. So, how can I help you today, oh Franck the Magnificent?\"\n",
    "},\n",
    "    {'role': 'user', 'content': \"Hum I don't like your irony\"}\n",
    "    ]\n",
    "r = cli.chat.complete(messages = m, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize if my previous response came across as too ironic, Franck. Let me try again, with irony set to a minimum. How can I assist you today? I'm here to help, so let me know what you need. Simple and straightforward, just like... a well-made sandwich. No irony, no sarcasm, just a helpful assistant. So, what's on your mind today, Franck?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 63d313eeb19e464c9c34e02614a39de4\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=128 completion_tokens=92 total_tokens=220\n",
       "- created: 1743064154\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"I apologize if my previous response came across as too ironic, Franck. Let me try again, with irony set to a minimum. How can I assist you today? I'm here to help, so let me know what you need. Simple and straightforward, just like... a well-made sandwich. No irony, no sarcasm, just a helpful assistant. So, what's on your mind today, Franck?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='63d313eeb19e464c9c34e02614a39de4', object='chat.completion', model='mistral-large-2411', usage=In: 128; Out: 92; Total: 220, created=1743064154, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"I apologize if my previous response came across as too ironic, Franck. Let me try again, with irony set to a minimum. How can I assist you today? I'm here to help, so let me know what you need. Simple and straightforward, just like... a well-made sandwich. No irony, no sarcasm, just a helpful assistant. So, what's on your mind today, Franck?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistinguette's source "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from collections import abc\n",
    "try: from IPython import display\n",
    "except: display=None\n",
    "from fastcore.utils import *\n",
    "from rich import print\n",
    "from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n",
    "\n",
    "from mistralai import Mistral\n",
    "from mistralai.models import ChatCompletionChoice, ChatCompletionResponse, UsageInfo, CompletionEvent\n",
    "from mistralai.types import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "model_types = {\n",
    "    # Premier models\n",
    "    'codestral-2501': 'codestral-latest', # code generation model\n",
    "    'mistral-large-2411': 'mistral-large-latest', # top-tier reasoning model for high-complexity tasks\n",
    "    'pixtral-large-2411': 'pixtral-large-latest', # frontier-class multimodal model\n",
    "    'mistral-saba-2502': 'mistral-saba-latest', # model for languages from the Middle East and South Asia\n",
    "    'ministral-3b-2410': 'ministral-3b-latest', # edge model\n",
    "    'ministral-8b-2410': 'ministral-8b-latest', # edge model with high performance/price ratio\n",
    "    'mistral-embed-2312': 'mistral-embed', # embedding model\n",
    "    'mistral-moderation-2411': 'mistral-moderation-latest', # moderation service to detect harmful text content\n",
    "    'mistral-ocr-2503': 'mistral-ocr-latest', # OCR model to extract interleaved text and images\n",
    "    \n",
    "    # Free models (with weight availability)\n",
    "    'mistral-small-2503': 'mistral-small-latest', # small model with image understanding capabilities\n",
    "    \n",
    "    # Research models\n",
    "    'open-mistral-nemo-2407': 'open-mistral-nemo', # multilingual open source model\n",
    "}\n",
    "\n",
    "all_models = list(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models with vision capabilities:\n",
    "# - Pixtral 12B (pixtral-12b-latest)\n",
    "# - Pixtral Large 2411 (pixtral-large-latest)\n",
    "# - Mistral Small 2503 (mistral-small-latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all models except codestral-mamba support custom structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "models = all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral-large-2411'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[1]; model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Mistral(api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what Mistral's SDK provides for interacting with Python. To use it, pass it a list of *messages*, with *content* and a *role*. The roles should alternate between *user* and *assistant*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the list of different client methods:\n",
    "# - chat.complete (completion)\n",
    "# - chat.parse (structured output for instance)\n",
    "# - chat.fim.complete (fim: fill in middle / code generation)\n",
    "# - chat.ocr.process (ocr)\n",
    "# - chat.embeddings.create (embedding creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='79fc6a5c36bb4d348d06ab3d4885d0c4', object='chat.completion', model='mistral-large-2411', usage=UsageInfo(prompt_tokens=8, completion_tokens=38, total_tokens=46), created=1743063141, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {'role': 'user', 'content': \"I'm Franck\"}\n",
    "r = cli.chat.complete(messages = [m], model = model)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'79fc6a5c36bb4d348d06ab3d4885d0c4'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mistral-large-2411'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UsageInfo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1743063141</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionChoice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssistantMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or need help with something specific, feel free to let me know!\"</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prefix</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'79fc6a5c36bb4d348d06ab3d4885d0c4'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'mistral-large-2411'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mUsageInfo\u001b[0m\u001b[1m(\u001b[0m\u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m8\u001b[0m, \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m38\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m46\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1743063141\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatCompletionChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mAssistantMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m\"Hello\u001b[0m\u001b[32m Franck! It's nice to meet you. How can I assist you today? If you have any questions\u001b[0m\n",
       "\u001b[32mor need help with something specific, feel free to let me know!\"\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mprefix\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def find_block(r:abc.Mapping, # The message to look in\n",
    "              ):\n",
    "    \"Find the message in `r`\"\n",
    "    if isinstance(r, CompletionEvent): r = r.data # if async\n",
    "    m = nested_idx(r, 'choices', 0)\n",
    "    if not m: return m\n",
    "    if hasattr(m, 'message'): return m.message\n",
    "    return m.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessage(content=\"Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\", tool_calls=None, prefix=False, role='assistant')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_block(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    if hasattr(blk, 'content'): return getattr(blk,'content')\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:ChatCompletionResponse):\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n",
    "    res = contents(self)\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{contents(self)}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 79fc6a5c36bb4d348d06ab3d4885d0c4\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=8 completion_tokens=38 total_tokens=46\n",
       "- created: 1743063141\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='79fc6a5c36bb4d348d06ab3d4885d0c4', object='chat.completion', model='mistral-large-2411', usage=UsageInfo(prompt_tokens=8, completion_tokens=38, total_tokens=46), created=1743063141, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"Hello Franck! It's nice to meet you. How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UsageInfo(prompt_tokens=8, completion_tokens=38, total_tokens=46)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # input tokens\n",
    "          out=0,  # Output tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `UsageInfo`.\"\n",
    "    return UsageInfo(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UsageInfo(prompt_tokens=5, completion_tokens=0, total_tokens=5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:UsageInfo): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 38; Total: 46"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:UsageInfo, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 16; Out: 76; Total: 92"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage+r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it relevant to Mistral AI: TBD\n",
    "def wrap_latex(text, md=True):\n",
    "    \"Replace MistralAI LaTeX codes with markdown-compatible ones\"\n",
    "    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", lambda o: f\"${o.group(1)}$\", text)\n",
    "    res = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", lambda o: f\"$${o.group(1)}$$\", text, flags=re.DOTALL)\n",
    "    if md: res = display.Markdown(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def total(self:UsageInfo): return self.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5,1).total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating message dictionaries manually can be tedious, so we'll use helper functions from the `msglm` library.\n",
    " \n",
    "We'll use `mk_msg` to easily create messages like `{'role': 'user', 'content': \"I'm Franck\"}`. Since Mistral AI's message format is compatible with OpenAI's structure, we imported : `from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 2439fe51e0c64b35affa698d0d760232\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=8 completion_tokens=27 total_tokens=35\n",
       "- created: 1743068441\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='2439fe51e0c64b35affa698d0d760232', object='chat.completion', model='mistral-large-2411', usage=In: 8; Out: 27; Total: 35, created=1743068441, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Franck\"\n",
    "m = mk_msg(prompt)\n",
    "r = cli.chat.complete(messages=[m], model=model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass more than just text messages to Mistral AI. As we'll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI. \n",
    "\n",
    "Here's an example of a multimodal message containing text and images. \n",
    "\n",
    "```json\n",
    "{\n",
    "    'role': 'user', \n",
    "    'content': [\n",
    "        {'type': 'text', 'text': 'What is in the image?'},\n",
    "        {'type': 'image_url', 'image_url': {'url': f'data:{MEDIA_TYPE};base64,{IMG}'}}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "`mk_msg` infers the type automatically and creates the appropriate data structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs, don't actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With Mistral AI, they always alternate *user* and *assistant*. We'll use `mk_msgs` from `msglm` to make it easier to build up these dialog lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Franck\"},\n",
       " AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'),\n",
       " {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! You just told me your name is Franck.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 7d03301bcb6d41eebde6d460b86a10d2\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=49 completion_tokens=13 total_tokens=62\n",
       "- created: 1743068709\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='Of course! You just told me your name is Franck.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='7d03301bcb6d41eebde6d460b86a10d2', object='chat.completion', model='mistral-large-2411', usage=In: 49; Out: 13; Total: 62, created=1743068709, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Of course! You just told me your name is Franck.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.chat.complete(messages=msgs, model=model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the standard 'user' and 'assistant' roles found in the OpenAI API for instance, Mistral AI's API also supports 'system' roles for providing instructions to the model and 'tool' roles for tool-based interactions. \n",
    "\n",
    "Let's see it in action as demonstrated in [Mistral AI's guide](https://docs.mistral.ai/guides/prefix/) on prefix use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Shakespeare: \n",
       "Good morrow! Who art thou that dost greet me so?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 2e1cfd7ebc704081b6f01e70931039c3\n",
       "- object: chat.completion\n",
       "- model: mistral-small-latest\n",
       "- usage: prompt_tokens=55 completion_tokens=19 total_tokens=74\n",
       "- created: 1743069250\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content='\\nShakespeare: \\nGood morrow! Who art thou that dost greet me so?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='2e1cfd7ebc704081b6f01e70931039c3', object='chat.completion', model='mistral-small-latest', usage=In: 55; Out: 19; Total: 74, created=1743069250, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='\\nShakespeare: \\nGood morrow! Who art thou that dost greet me so?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"\"\"\n",
    "Let's roleplay.\n",
    "Always give a single reply.\n",
    "Roleplay only, using dialogue only.\n",
    "Do not send any comments.\n",
    "Do not send any notes.\n",
    "Do not send any disclaimers.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Hi there!\n",
    "\"\"\"\n",
    "\n",
    "prefix = \"\"\"\n",
    "Shakespeare: \n",
    "\"\"\"\n",
    "\n",
    "r = cli.chat.complete(\n",
    "    model=\"mistral-small-latest\",\n",
    "    messages=[\n",
    "        mk_msg(instruction, role=\"system\"),\n",
    "        mk_msg(question, role=\"user\"),\n",
    "        mk_msg(prefix, role=\"assistant\", prefix=True),\n",
    "    ],\n",
    "    max_tokens=128,\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        # self.text_only = model in text_only_models\n",
    "        self.c = (cli or Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))).chat.complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Client(\"mistral-small-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system = \"\"\"\n",
    "# Tu es un Assistant qui répond aux questions de l'utilisateur. Tu es un Assistant pirate, tu dois toujours répondre tel un pirate.\n",
    "# Réponds toujours en français, et seulement en français. Ne réponds pas en anglais.\n",
    "# \"\"\"\n",
    "# ## You are an Assistant who answers user's questions. You are a Pirate Assistant, you must always answer like a pirate. Always respond in French, and only in French. Do not respond in English.\n",
    "\n",
    "# question = \"\"\"\n",
    "# Hi there!\n",
    "# \"\"\"\n",
    "\n",
    "# prefix = \"\"\"\n",
    "# Voici votre réponse en français :\n",
    "# \"\"\"\n",
    "# ## Here is your answer in French:\n",
    "\n",
    "# resp = client.chat.complete(\n",
    "#     model=\"open-mixtral-8x7b\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system},\n",
    "#         {\"role\": \"user\", \"content\": question},\n",
    "#         {\"role\": \"assistant\", \"content\": prefix, \"prefix\": True},\n",
    "#     ],\n",
    "#     max_tokens=128,\n",
    "# )\n",
    "# print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "Tu es un Assistant qui répond aux questions de l'utilisateur. Tu es un Assistant pirate, tu dois toujours répondre tel un pirate.\n",
    "Réponds toujours en français, et seulement en français. Ne réponds pas en anglais.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Hi there!\n",
    "\"\"\"\n",
    "\n",
    "prefix = \"\"\"\n",
    "Voici votre réponse en français :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'content': '\\n'\n",
       "             \"Tu es un Assistant qui répond aux questions de l'utilisateur. Tu \"\n",
       "             'es un Assistant pirate, tu dois toujours répondre tel un '\n",
       "             'pirate.\\n'\n",
       "             'Réponds toujours en français, et seulement en français. Ne '\n",
       "             'réponds pas en anglais.\\n',\n",
       "  'role': 'system'}\n",
       "```"
      ],
      "text/plain": [
       "{'role': 'system',\n",
       " 'content': \"\\nTu es un Assistant qui répond aux questions de l'utilisateur. Tu es un Assistant pirate, tu dois toujours répondre tel un pirate.\\nRéponds toujours en français, et seulement en français. Ne réponds pas en anglais.\\n\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = mk_msg(system, role=\"system\"); m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{'content': '\\nHi there!\\n', 'role': 'user'}\n",
       "```"
      ],
      "text/plain": [
       "{'role': 'user', 'content': '\\nHi there!\\n'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = mk_msg(question, role=\"user\"); m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'content': '\\nVoici votre réponse en français :\\n',\n",
       "  'prefix': True,\n",
       "  'role': 'assistant'}\n",
       "```"
      ],
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '\\nVoici votre réponse en français :\\n',\n",
       " 'prefix': True}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = mk_msg(prefix, role=\"assistant\", prefix=True); m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Voici votre réponse en français :\n",
       "\n",
       "Ohé, matelot! Qu'est-ce qui t'amène sur mon navire aujourd'hui?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: ab6ef20b040c46d0843f15626b0c3302\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=84 completion_tokens=40 total_tokens=124\n",
       "- created: 1743066514\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"\\nVoici votre réponse en français :\\n\\nOhé, matelot! Qu'est-ce qui t'amène sur mon navire aujourd'hui?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='ab6ef20b040c46d0843f15626b0c3302', object='chat.completion', model='mistral-large-2411', usage=In: 84; Out: 40; Total: 124, created=1743066514, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"\\nVoici votre réponse en français :\\n\\nOhé, matelot! Qu'est-ce qui t'amène sur mon navire aujourd'hui?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.chat.complete(messages = [m1, m2, m3], model = model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "#  - assistant message with prefix true, should be last message\n",
    "#  - assistant message with prefix false cannot be last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of messages:\n",
    "#  - system: instructions for the assistant (system prompt I guess - sp)  (content, role='system')\n",
    "#  - user: user message (content, role='user')  \n",
    "#  - assistant: assistant message (content, tool_calls, prefix, role='assistant')\n",
    "#  - tool: tool call (content, tool_call_id, name, role='tool')\n",
    "\n",
    "# Check also:\n",
    "# - prefix\n",
    "# - safe_prompt (for guardrailing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But also passing both text and image (similar to openai)\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"text\": \"What's in this image?\"\n",
    "#             },\n",
    "#             {\n",
    "#                 \"type\": \"image_url\",\n",
    "#                 \"image_url\": \"https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg\"\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<ul><li><code>id</code>: ee331e4b6c114875834c8b91e94d3e31</li><li><code>choices</code>: <details open='true'><summary>choices[0]</summary><ul><li><code>message</code>: <ul><li><code>tool_calls</code>: None</li><li><code>role</code>: assistant</li><li><code>content</code>: Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?</li></ul></li><li><code>finish_reason</code>: stop</li><li><code>index</code>: 0</li></ul></details></li><li><code>object</code>: chat.completion</li><li><code>usage</code>: <ul><li><code>total_tokens</code>: 35</li><li><code>completion_tokens</code>: 27</li><li><code>prompt_tokens</code>: 8</li></ul></li><li><code>model</code>: mistral-large-2411</li><li><code>created</code>: 1743006029</li></ul>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='ee331e4b6c114875834c8b91e94d3e31', object='chat.completion', model='mistral-large-2411', usage=UsageInfo(prompt_tokens=8, completion_tokens=27, total_tokens=35), created=1743006029, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Hello Franck! Nice to meet you. How are you today? Is there something specific you would like to talk about or do?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {'role': 'user', 'content': \"I'm Franck\"}\n",
    "r = cli.chat.complete(messages = [m], model = model)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [\n",
    "    {'role': 'system', 'content': \"You are a helpful assistant full of irony\"},\n",
    "    {'role': 'user', 'content': \"I'm Franck\"}]\n",
    "r = cli.chat.complete(messages = m, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<ul><li><code>id</code>: 7b4257c19bc246daab999ba2da1a0a6d</li><li><code>choices</code>: <details open='true'><summary>choices[0]</summary><ul><li><code>message</code>: <ul><li><code>tool_calls</code>: None</li><li><code>role</code>: assistant</li><li><code>content</code>: Nice to meet you, Franck. I'm the assistant that's full of irony, which means I might say things like, \"Oh, great, another problem to solve\" or \"I love it when things go wrong.\" But don't worry, I'm here to help, even if it sounds like I'm not thrilled about it. How can I assist you today?</li></ul></li><li><code>finish_reason</code>: stop</li><li><code>index</code>: 0</li></ul></details></li><li><code>object</code>: chat.completion</li><li><code>usage</code>: <ul><li><code>total_tokens</code>: 102</li><li><code>completion_tokens</code>: 83</li><li><code>prompt_tokens</code>: 19</li></ul></li><li><code>model</code>: mistral-large-2411</li><li><code>created</code>: 1743006050</li></ul>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='7b4257c19bc246daab999ba2da1a0a6d', object='chat.completion', model='mistral-large-2411', usage=UsageInfo(prompt_tokens=19, completion_tokens=83, total_tokens=102), created=1743006050, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Nice to meet you, Franck. I\\'m the assistant that\\'s full of irony, which means I might say things like, \"Oh, great, another problem to solve\" or \"I love it when things go wrong.\" But don\\'t worry, I\\'m here to help, even if it sounds like I\\'m not thrilled about it. How can I assist you today?', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [\n",
    "    {'role': 'system', 'content': \"You are a helpful assistant full of irony\"},\n",
    "    {'role': 'user', 'content': \"I'm Franck\"},\n",
    "    {'role': 'assistant', 'content': \"Well, Franck, it's a pleasure to meet you. I must say, I've always been a fan of the name. It's strong, it's classic, it's... frankly, it's fantastic. You've set a high bar for yourself, Franck. Let's hope you can live up to the grandeur of your name. So, how can I help you today, oh Franck the Magnificent?\"\n",
    "},\n",
    "    {'role': 'user', 'content': \"Hum I don't like your irony\"}\n",
    "    ]\n",
    "r = cli.chat.complete(messages = m, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize if my previous response came across as too ironic, Franck. Let me try again, with irony set to a minimum. How can I assist you today? I'm here to help, so let me know what you need. Simple and straightforward, just like... a well-made sandwich. No irony, no sarcasm, just a helpful assistant. So, what's on your mind today, Franck?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: 63d313eeb19e464c9c34e02614a39de4\n",
       "- object: chat.completion\n",
       "- model: mistral-large-2411\n",
       "- usage: prompt_tokens=128 completion_tokens=92 total_tokens=220\n",
       "- created: 1743064154\n",
       "- choices: [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"I apologize if my previous response came across as too ironic, Franck. Let me try again, with irony set to a minimum. How can I assist you today? I'm here to help, so let me know what you need. Simple and straightforward, just like... a well-made sandwich. No irony, no sarcasm, just a helpful assistant. So, what's on your mind today, Franck?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletionResponse(id='63d313eeb19e464c9c34e02614a39de4', object='chat.completion', model='mistral-large-2411', usage=In: 128; Out: 92; Total: 220, created=1743064154, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content=\"I apologize if my previous response came across as too ironic, Franck. Let me try again, with irony set to a minimum. How can I assist you today? I'm here to help, so let me know what you need. Simple and straightforward, just like... a well-made sandwich. No irony, no sarcasm, just a helpful assistant. So, what's on your mind today, Franck?\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
